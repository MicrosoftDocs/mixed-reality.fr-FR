---
title: Manipulation directe
description: Vue d’ensemble du modèle d’entrée de manipulation directe
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/02/2019
ms.topic: article
keywords: Mixte réalité, les regards, les regards ciblant, interaction, concevoir
ms.openlocfilehash: d855955d44c1cf074849992e5dd7b36b54675fdd
ms.sourcegitcommit: f5c1dedb3b9e29f27f627025b9e7613931a7ce18
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 04/27/2019
ms.locfileid: "64581329"
---
# <a name="direct-manipulation"></a><span data-ttu-id="4991a-104">Manipulation directe</span><span class="sxs-lookup"><span data-stu-id="4991a-104">Direct manipulation</span></span>
<span data-ttu-id="4991a-105">Manipulation directe est un modèle d’entrée qui implique de toucher hologrammes directement avec vos mains.</span><span class="sxs-lookup"><span data-stu-id="4991a-105">Direct manipulation is an input model that involves touching holograms directly with your hands.</span></span> <span data-ttu-id="4991a-106">L’objectif de la manipulation directe est que les objets se comportent exactement comme dans le monde réel.</span><span class="sxs-lookup"><span data-stu-id="4991a-106">The goal with direct manipulation is that objects behave just as they do in the real world.</span></span> <span data-ttu-id="4991a-107">Boutons peuvent être activées simplement en appuyant sur les objets peuvent être interceptés par s’emparer les et contenu 2D se comporte comme un écran tactile virtuel.</span><span class="sxs-lookup"><span data-stu-id="4991a-107">Buttons can be activated simply by pressing them, objects can be picked up by grabbing them, and 2D content behaves like a virtual touchscreen.</span></span>  <span data-ttu-id="4991a-108">Pour cette raison, diriger manipulation est facile pour les utilisateurs pour en savoir plus, et il est passionnant trop.</span><span class="sxs-lookup"><span data-stu-id="4991a-108">Because of this, direct manipulation is easy for users to learn, and it's fun too.</span></span>  <span data-ttu-id="4991a-109">Il est considéré comme un modèle d’entrée « proches », ce qui signifie qu’il est particulièrement adapté pour l’interaction avec du contenu qui est au sein des armes atteindre.</span><span class="sxs-lookup"><span data-stu-id="4991a-109">It is considered a "near" input model, meaning it is best used for interacting with content that is within arms reach.</span></span>

<span data-ttu-id="4991a-110">Un élément clé qui facilite la manipulation directe pour en savoir est qu’il est basé sur le caractère intuitif.</span><span class="sxs-lookup"><span data-stu-id="4991a-110">A key ingredient that makes direct manipulation easy to learn is that it is affordance-based.</span></span> <span data-ttu-id="4991a-111">Il n’y a aucune mouvements symboliques à apprendre aux utilisateurs.</span><span class="sxs-lookup"><span data-stu-id="4991a-111">There are no symbolic gestures to teach users.</span></span> <span data-ttu-id="4991a-112">Toutes les interactions doivent être générées autour d’un élément visuel qui peut être touché ou saisi.</span><span class="sxs-lookup"><span data-stu-id="4991a-112">All interactions should be built around a visual element that can be touched or grabbed.</span></span>

## <a name="device-support"></a><span data-ttu-id="4991a-113">Prise en charge des appareils</span><span class="sxs-lookup"><span data-stu-id="4991a-113">Device support</span></span>

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><span data-ttu-id="4991a-114"><strong>Modèle d’entrée</strong></span><span class="sxs-lookup"><span data-stu-id="4991a-114"><strong>Input model</strong></span></span></td>
        <td><span data-ttu-id="4991a-115"><a href="hololens-hardware-details.md"><strong>HoloLens (1er gen)</strong></a></span><span class="sxs-lookup"><span data-stu-id="4991a-115"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
        <td><span data-ttu-id="4991a-116"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="4991a-116"><strong>HoloLens 2</strong></span></span></td>
        <td><span data-ttu-id="4991a-117"><a href="immersive-headset-hardware-details.md"><strong>Casques IMMERSIFS</strong></a></span><span class="sxs-lookup"><span data-stu-id="4991a-117"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
    </tr>
     <tr>
        <td><span data-ttu-id="4991a-118">Manipulation directe (près l’interaction de la main)</span><span class="sxs-lookup"><span data-stu-id="4991a-118">Direct manipulation (Near hand interaction)</span></span></td>
        <td><span data-ttu-id="4991a-119">❌ Ne pas pris en charge</span><span class="sxs-lookup"><span data-stu-id="4991a-119">❌ Not supported</span></span></td>
        <td><span data-ttu-id="4991a-120">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="4991a-120">✔️ Recommended</span></span></td>
        <td><span data-ttu-id="4991a-121">➕ Une autre option mais <a href="point-and-commit.md">Point et validation (interaction lointain)</a> est recommandé</span><span class="sxs-lookup"><span data-stu-id="4991a-121">➕ An alternate option but <a href="point-and-commit.md">Point and commit (far interaction)</a> is recommended</span></span></td>
    </tr>
</table>

<span data-ttu-id="4991a-122">Manipulation directe est un modèle d’entrée principal sur HoloLens 2, utilisant la main articulée nouveau système de suivi.</span><span class="sxs-lookup"><span data-stu-id="4991a-122">Direct manipulation is a primary input model on HoloLens 2, utilizing the new articulated hand tracking system.</span></span> <span data-ttu-id="4991a-123">Le modèle d’entrée est également disponible sur des casques IMMERSIFS grâce à l’utilisation de contrôleurs de mouvement, mais n’est pas recommandé que principal moyen d’interaction en dehors de la manipulation des objets.</span><span class="sxs-lookup"><span data-stu-id="4991a-123">The input model is also available on immersive headsets through the use of motion controllers, but is not recommended a primary means of interaction outside of object manipulation.</span></span>  <span data-ttu-id="4991a-124">Manipluation directe n’est pas disponible sur HoloLens v1.</span><span class="sxs-lookup"><span data-stu-id="4991a-124">Direct manipluation is not available on HoloLens v1.</span></span>

## <a name="collidable-fingertip"></a><span data-ttu-id="4991a-125">Bout des doigts collidable</span><span class="sxs-lookup"><span data-stu-id="4991a-125">Collidable fingertip</span></span>
<span data-ttu-id="4991a-126">Sur HoloLens 2, mains réel de l’utilisateur sont reconnus et interprétés en tant que modèles squelettes gauche et droite.</span><span class="sxs-lookup"><span data-stu-id="4991a-126">On HoloLens 2, user's real hands are recognized and interpreted as left and right hand skeletal models.</span></span> <span data-ttu-id="4991a-127">Pour implémenter l’idée de toucher hologrammes directement des mains, dans l’idéal, 5 colliders ne sont attachés à portée de 5 main de chaque modèle de squelette de main.</span><span class="sxs-lookup"><span data-stu-id="4991a-127">To implement the idea of touching holograms directly with hands, ideally, 5 colliders could be attached to 5 fingertips of each hand skeletal model.</span></span> <span data-ttu-id="4991a-128">Toutefois, en pratique, en raison du manque de rétroaction tactile, portée de main collidable 10 entraîne un grand nombre de collisions inattendus et imprévisibles avec hologrammes.</span><span class="sxs-lookup"><span data-stu-id="4991a-128">However, practically, due to the lack of tactile feedback, 10 collidable fingertips cause lots of unexpected and unpredictable collisions with holograms.</span></span> <span data-ttu-id="4991a-129">Par conséquent, nous vous suggérons pour ne placer un collider sur chaque doigt de l’index.</span><span class="sxs-lookup"><span data-stu-id="4991a-129">Hence, we suggest to only put a collider on each index finger.</span></span> <span data-ttu-id="4991a-130">L’index collidable portée de main peut toujours servir de points de contact active pour diverses entrées tactiles multipoints impliquant autre doigt, telles que press 1 doigt, 1 doigt appuyez sur 2 Appuyez sur le doigt et appuyez sur 5 doigt.</span><span class="sxs-lookup"><span data-stu-id="4991a-130">The collidable index fingertips can still serve as active touch points for diverse touch gestures involving other fingers, such as 1 finger press, 1 finger tap, 2 finger press and 5 finger press.</span></span>

![Image de collidable par un doigt](images/Collidable-Fingertip-720px.jpg)<br>

### <a name="sphere-collider"></a><span data-ttu-id="4991a-132">Sphère collider</span><span class="sxs-lookup"><span data-stu-id="4991a-132">Sphere collider</span></span>
<span data-ttu-id="4991a-133">Au lieu d’utiliser la forme générique aléatoire, nous vous suggérons d’utiliser un collider sphère et rendre visuellement pour fournir une meilleure signaux pour le ciblage de quasiment.</span><span class="sxs-lookup"><span data-stu-id="4991a-133">Instead of using random generic shape, we suggest to use a sphere collider and to visually render it to provide better cues for near targeting.</span></span> <span data-ttu-id="4991a-134">Diamètre de la sphère doit correspondre à l’épaisseur du doigt index pour augmenter la précision de tactile.</span><span class="sxs-lookup"><span data-stu-id="4991a-134">The sphere's diameter should match the thickness of the index finger to increase touch accuracy.</span></span> <span data-ttu-id="4991a-135">Il sera facile de récupérer la variable d’épaisseur du doigt en appelant l’aiguille de l’API.</span><span class="sxs-lookup"><span data-stu-id="4991a-135">It will be easy to retrieve the variable of finger thickness by calling the hand API.</span></span>

<br>

### <a name="fingertip-cursor"></a><span data-ttu-id="4991a-136">Curseur du bout des doigts</span><span class="sxs-lookup"><span data-stu-id="4991a-136">Fingertip cursor</span></span>
<span data-ttu-id="4991a-137">Outre le rendu d’une sphère collidable sur le bout des doigts index, nous créons une solution d’avance, le curseur par un doigt, pour obtenir une meilleure près de l’expérience de ciblage de manière interactive.</span><span class="sxs-lookup"><span data-stu-id="4991a-137">In addition to rendering a collidable sphere on the index fingertip, we create an advance solution, fingertip cursor, to achieve better near targeting experience interactively.</span></span> <span data-ttu-id="4991a-138">Il est un curseur de forme de graphique en anneau attaché sur le bout des doigts index.</span><span class="sxs-lookup"><span data-stu-id="4991a-138">It is a donut shape cursor attached on the index fingertip.</span></span> <span data-ttu-id="4991a-139">En fonction de la proximité, il dynamiquement réagit à une cible en termes de l’orientation et la taille comme indiqué ci-dessous :</span><span class="sxs-lookup"><span data-stu-id="4991a-139">According to proximity, it dynamically reacts to a target in term of orientation and size as below:</span></span>
* <span data-ttu-id="4991a-140">Lorsque votre index se déplace vers un hologramme, le curseur est toujours parallèle à la surface de l’hologramme et progressivement diminue sa taille en conséquence.</span><span class="sxs-lookup"><span data-stu-id="4991a-140">When an index finger moves toward a hologram, the cursor is always parallel to the surface of the hologram and gradually shrinks its size accordingly.</span></span> 
* <span data-ttu-id="4991a-141">Dès que le doigt toucher la surface, le curseur est réduit à un point et émet un événement tactile.</span><span class="sxs-lookup"><span data-stu-id="4991a-141">As soon as the finger touch the surface, the cursor shrinks into a dot and emits a touch event.</span></span>

<br> <span data-ttu-id="4991a-142">Avec les commentaires interactive, les utilisateurs peuvent obtenir une haute précision près de ciblage des tâches, telles que le déclenchement d’un lien hypertexte sur un contenu web ou en appuyant sur un bouton.</span><span class="sxs-lookup"><span data-stu-id="4991a-142">With the interactive feedback, users can achieve high precision near targeting tasks, such as triggering a hyperlink on a web content or pressing a button.</span></span> <br>

![Image de curseur par un doigt](images/Fingertip-Cursor-720px.jpg)<br>

## <a name="bounding-box-with-proximity-shader"></a><span data-ttu-id="4991a-144">Zone englobante avec le nuanceur de proximité</span><span class="sxs-lookup"><span data-stu-id="4991a-144">Bounding box with proximity shader</span></span>
<span data-ttu-id="4991a-145">L’hologramme lui-même nécessite également de fournir des commentaires visuels et audio pour compenser l’absence de rétroaction tactile.</span><span class="sxs-lookup"><span data-stu-id="4991a-145">The hologram itself also requires to provide both visual and audio feedbacks to compensate the lack of tactile feedback.</span></span> <span data-ttu-id="4991a-146">Pour ce faire, nous générons le concept du cadre englobant avec le nuanceur de proximité.</span><span class="sxs-lookup"><span data-stu-id="4991a-146">For that, we generate the concept of bounding box with proximity shader.</span></span> <span data-ttu-id="4991a-147">Un rectangle englobant est une zone volumétriques minimales qui englobe un objet 3D.</span><span class="sxs-lookup"><span data-stu-id="4991a-147">A bounding box is a minimun volumetric area that encloses a 3D object.</span></span> <span data-ttu-id="4991a-148">La zone englobante a un mécanisme interactive de rendu appelé nuanceur de proximité.</span><span class="sxs-lookup"><span data-stu-id="4991a-148">The bounding box has an interactive rendering mechanism called proximity shader.</span></span> <span data-ttu-id="4991a-149">Le nuanceur de proximité se comporte comme suit :</span><span class="sxs-lookup"><span data-stu-id="4991a-149">The proximity shader behaves as below:</span></span>

* <span data-ttu-id="4991a-150">Lorsque le doigt de l’index se trouve dans une plage, un coup de projecteur par un doigt est converti sur l’aire du cadre englobant.</span><span class="sxs-lookup"><span data-stu-id="4991a-150">When the index finger is within a range, a fingertip spotlight is cast on the surface of bounding box.</span></span> 
* <span data-ttu-id="4991a-151">Lorsque le bout des doigts se rapproche la surface, actualités condense en conséquence.</span><span class="sxs-lookup"><span data-stu-id="4991a-151">When the fingertip gets closer to the surface, the spotlight condenses accordingly.</span></span> 
* <span data-ttu-id="4991a-152">Dès que le bout des doigts toucher la surface, la totalité de zone englobante modifie la couleur ou générer un effet visuel pour refléter l’état tactile.</span><span class="sxs-lookup"><span data-stu-id="4991a-152">As soon as the fingertip touch the surface, the whole bounding box changes the color or generate visual effect to reflect the touch state.</span></span> 
* <span data-ttu-id="4991a-153">Pendant ce temps, un effet audio peut être activé pour améliorer le feedback de saisie visual.</span><span class="sxs-lookup"><span data-stu-id="4991a-153">Meanwhile, a sound effect can be activated to enhance the visual touch feedback.</span></span>

![Zone englobante avec l’image de nuanceur de proximité](images/Bounding-Box-With-Proximity-Shader-720px.jpg)<br>

## <a name="pressable-button"></a><span data-ttu-id="4991a-155">Bouton PRESSEE</span><span class="sxs-lookup"><span data-stu-id="4991a-155">Pressable button</span></span>
<span data-ttu-id="4991a-156">Avec un par un doigt collidable, les utilisateurs sont maintenant prêtes à interagir avec le composant interface utilisateur de HOLOGRAPHIQUE très fondamental, bouton PRESSEE.</span><span class="sxs-lookup"><span data-stu-id="4991a-156">With a collidable fingertip, users are now ready to interact with the very fundamental holographic UI component, pressable button.</span></span> <span data-ttu-id="4991a-157">Un bouton PRESSEE est un bouton HOLOGRAPHIQUE adapté pour press du doigt direct.</span><span class="sxs-lookup"><span data-stu-id="4991a-157">A pressable button is a holographic button tailored for direct finger press.</span></span> <span data-ttu-id="4991a-158">Là encore, en raison du manque de rétroaction tactile, un bouton PRESSEE doter les deux mécanismes pour aborder la rétroaction tactile les problèmes liés.</span><span class="sxs-lookup"><span data-stu-id="4991a-158">Again, due to the lack of tactile feedback, a pressable button equips a couple mechanisms to tackle tactile feedback related issues.</span></span> 
* <span data-ttu-id="4991a-159">Le premier mécanisme est englobant avec le nuanceur de proximité, ce qui a déjà été résolu dans le paragraphe qui précède.</span><span class="sxs-lookup"><span data-stu-id="4991a-159">The first mechanism is bounding box with proximity shader, which has already been addressed in the foregoing paragraph.</span></span> <span data-ttu-id="4991a-160">Il sert à fournir une meilleure idée de proximité permettant aux utilisateurs de l’approche et assurez-vous contact avec un bouton.</span><span class="sxs-lookup"><span data-stu-id="4991a-160">It serves to provide better sense of proximity for users to approach and make contact with a button.</span></span> 
* <span data-ttu-id="4991a-161">Le second est DÉPRESSION.</span><span class="sxs-lookup"><span data-stu-id="4991a-161">The second one is depression.</span></span> <span data-ttu-id="4991a-162">Il crée le sens de presse, après qu’un par un doigt contacte le bouton.</span><span class="sxs-lookup"><span data-stu-id="4991a-162">It creates sense of press, after a fingertip contacts the button.</span></span> <span data-ttu-id="4991a-163">Le mécanisme est que le bouton se déplace étroitement avec le bout des doigts sur l’axe de profondeur.</span><span class="sxs-lookup"><span data-stu-id="4991a-163">The mechanism is that the button tightly moves with the fingertip along the depth axis.</span></span> <span data-ttu-id="4991a-164">Le bouton peut être déclenché dès qu’atteindre une profondeur désignée (sur Presse) ou en laissant la profondeur (sur la mise en production) après le passage par son intermédiaire.</span><span class="sxs-lookup"><span data-stu-id="4991a-164">The button can be triggered as soon as reaching a designated depth (on press) or leaving the depth (on release) after passing through it.</span></span> 
* <span data-ttu-id="4991a-165">L’effet audio doit être ajouté pour améliorer les commentaires, lorsque le bouton est déclenché.</span><span class="sxs-lookup"><span data-stu-id="4991a-165">The sound effect should be added to enhance feedback, when the button is triggered.</span></span> 

![Image du bouton PRESSEE](images/Pressable-Button-720px.jpg)<br>

## <a name="2d-slate-interaction"></a><span data-ttu-id="4991a-167">Interaction ardoise 2D</span><span class="sxs-lookup"><span data-stu-id="4991a-167">2D slate interaction</span></span>
<span data-ttu-id="4991a-168">Une ardoise 2D est un conteneur HOLOGRAPHIQUE hébergement de contenu application 2D, comme navigateur web.</span><span class="sxs-lookup"><span data-stu-id="4991a-168">A 2D slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="4991a-169">Le concept de conception permettant d’interagir avec une ardoise 2D via la manipulation directe consiste à tirer parti du modèle mental de l’interaction avec un écran tactile physique.</span><span class="sxs-lookup"><span data-stu-id="4991a-169">The design concept for interacting with a 2D slate via direct manipulation is to leverage the mental model of interacting with a physical touch screen.</span></span><br> <br>
<span data-ttu-id="4991a-170">Pour l’interaction avec le contact ardoise :</span><span class="sxs-lookup"><span data-stu-id="4991a-170">For interacting with the slate contact:</span></span><br> 
* <span data-ttu-id="4991a-171">Les utilisateurs utiliser votre index à appuyer sur un bouton ou un lien hypertexte.</span><span class="sxs-lookup"><span data-stu-id="4991a-171">Users use an index finger to press a hyperlink or a button.</span></span> 
* <span data-ttu-id="4991a-172">Les utilisateurs utiliser votre index pour faire défiler un contenu ardoise diminué.</span><span class="sxs-lookup"><span data-stu-id="4991a-172">Users use an index finger to scroll a slate content up and down.</span></span> 
* <span data-ttu-id="4991a-173">Les utilisateurs utiliser deux doigts de l’index pour effectuer un zoom et de sortie au contenu ardoise selon un mouvement relatif des doigts.</span><span class="sxs-lookup"><span data-stu-id="4991a-173">Users use two index fingers to zoom in and out the slate content according to relative motion of fingers.</span></span> 
<span data-ttu-id="4991a-174">![Image d’ardoise 2D](images/2D-Slate-Interaction-720px.jpg)</span><span class="sxs-lookup"><span data-stu-id="4991a-174">![2D slate image](images/2D-Slate-Interaction-720px.jpg)</span></span><br>

<br><span data-ttu-id="4991a-175">Pour la manipulation 2D d’ardoise lui-même :</span><span class="sxs-lookup"><span data-stu-id="4991a-175">For manipulating the 2D slate itself:</span></span><br>
* <span data-ttu-id="4991a-176">Les utilisateurs peut permettre d’approcher les mains vers des angles et des bords pour révéler l’intuitivité de manipulation le plus proche.</span><span class="sxs-lookup"><span data-stu-id="4991a-176">Users can approach their hands toward corners and edges to reveal the closest manipulation affordances.</span></span> 
* <span data-ttu-id="4991a-177">En saisissant l’intuitivité de manipulation, les utilisateurs peuvent effectuer la mise à l’échelle uniforme via l’affordnaces coin et redistribution via l’intuitivité edge.</span><span class="sxs-lookup"><span data-stu-id="4991a-177">By grabbing the manipulation affordances, users can perform uniform scaling through the corner affordnaces and reflow via the edge affordances.</span></span> 
* <span data-ttu-id="4991a-178">En saisissant le holobar en haut de l’ardoise 2D peuvent utilisateurs déplacer l’ardoise entière.</span><span class="sxs-lookup"><span data-stu-id="4991a-178">Grabbing the holobar at the top of the 2D slate can users move the whole slate.</span></span><br><br>

![Image d’ardoise manipulation](images/Manipulate-2d-slate-720px.jpg)


## <a name="3d-object-manipulation"></a><span data-ttu-id="4991a-180">Manipulation des objets 3D</span><span class="sxs-lookup"><span data-stu-id="4991a-180">3D object manipulation</span></span>
<span data-ttu-id="4991a-181">HoloLens 2, les utilisateurs sont activés pour utiliser les mains pour manipuler directement les objets 3D hologramphic en appliquant un cadre englobant à chaque objet 3D.</span><span class="sxs-lookup"><span data-stu-id="4991a-181">In HoloLens 2, users are enabled to use their hands to direct manipulate 3D hologramphic objects by applying a bounding box to each 3D object.</span></span> <span data-ttu-id="4991a-182">La zone englobante fournit une meilleure perception de profondeur via son nuanceur de proximité.</span><span class="sxs-lookup"><span data-stu-id="4991a-182">The bounding box provides better depth perception through its proximity shader.</span></span> <span data-ttu-id="4991a-183">Avec la zone englobante, il existe deux approches de conception pour la manipulation des objets 3D :</span><span class="sxs-lookup"><span data-stu-id="4991a-183">With the bounding box, there are two design approaches for 3D object manipulation:</span></span>      
### <a name="affordance-based-manipulation"></a><span data-ttu-id="4991a-184">Le caractère intuitif en fonction de manipulation :</span><span class="sxs-lookup"><span data-stu-id="4991a-184">Affordance based manipulation:</span></span>
<span data-ttu-id="4991a-185">C’est un moyen permettant aux utilisateurs de manipuler l’objet 3D via la boîte englobante et l’intuitivité de manipulation autour d’elle.</span><span class="sxs-lookup"><span data-stu-id="4991a-185">It is a way for users to manipulate the 3D object through bounding box and the manipulation affordances around it.</span></span> <span data-ttu-id="4991a-186">Dès que la part de l’utilisateur est proche d’un objet 3D, la zone englobante et la plus proche intuitif sont dévoilées.</span><span class="sxs-lookup"><span data-stu-id="4991a-186">As soon as a user's hand is close to a 3D object, the bounding box and the nearest affordance are revealed.</span></span> <span data-ttu-id="4991a-187">Les utilisateurs peuvent saisir la zone englobante pour déplacer l’objet entier, l’intuitivité edge pour faire pivoter et le coner permettant à l’échelle de manière uniforme.</span><span class="sxs-lookup"><span data-stu-id="4991a-187">Users can grab the bounding box to move the whole object, the edge affordances to rotate and the coner affordances to scale uniformly.</span></span><br>

![Image de manipulation d’objet 3D](images/3D-Object-Manipulation-720px.jpg)<br>

### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="4991a-189">Le caractère non-intuitif en fonction de manipulation :</span><span class="sxs-lookup"><span data-stu-id="4991a-189">Non-affordance based manipulation:</span></span>
<span data-ttu-id="4991a-190">Dans cette mechanisom, aucun intuitif n’est attaché à la zone englobante.</span><span class="sxs-lookup"><span data-stu-id="4991a-190">In this mechanisom, no affordance is attached to the bounding box.</span></span> <span data-ttu-id="4991a-191">Les utilisateurs peuvent uniquement afficher la zone englobante, puis interagir directement avec lui.</span><span class="sxs-lookup"><span data-stu-id="4991a-191">Users can only reveal the bounding box, then directly interact with it.</span></span> <span data-ttu-id="4991a-192">Si la zone englobante est saisie avec une main, la traduction et la rotation de l’objet sont associés au mouvement et l’orientation de la main.</span><span class="sxs-lookup"><span data-stu-id="4991a-192">If the bounding box is grabbed with one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="4991a-193">Lorsque l’objet est saisi avec deux mains, les utilisateurs peuvent traduire, mettre à l’échelle et faire pivoter en fonction des mouvements relatifs de deux mains.</span><span class="sxs-lookup"><span data-stu-id="4991a-193">When the object is grabbed with two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span><br><br> 

<br><br>
<span data-ttu-id="4991a-194">Pour la manipulation exige la précision, nous vous recommandons afforance basé manipulation, en fournissant un niveau élevé de granularité.</span><span class="sxs-lookup"><span data-stu-id="4991a-194">For manipulation requires precision, we recommend afforance based manipulation, providing high level of granularity.</span></span> <span data-ttu-id="4991a-195">Pour une manipulation flexible, non intuitif manipulation sera un bon choix, offrant aux utilisateurs des expériences instantanés et amusant.</span><span class="sxs-lookup"><span data-stu-id="4991a-195">For flexible manipulation, non-affordance manipulation will be a good choice, offering users instant and playful experiences.</span></span>


## <a name="instinctual-gestures"></a><span data-ttu-id="4991a-196">Mouvements instinctual</span><span class="sxs-lookup"><span data-stu-id="4991a-196">Instinctual gestures</span></span>
<span data-ttu-id="4991a-197">Contrairement à HoloLens (1er gen), mouvements utilisateurs enseignement quelques prédéfinis, tels que Bloom et appuyez sur Air, HoloLens 2, nous ne demandons aux utilisateurs de mémoriser tout mouvement symbolique.</span><span class="sxs-lookup"><span data-stu-id="4991a-197">Unlike HoloLens (1st gen), teaching users a couple predefined gestures, such as Bloom and Air Tap, in HoloLens 2, we don't ask users to memorize any symbolic gesture.</span></span> <span data-ttu-id="4991a-198">Tous les gestes que les utilisateurs ont besoin pour interagir avec hologrammes et le contenu sont instinctual.</span><span class="sxs-lookup"><span data-stu-id="4991a-198">All gestures that users need for interacting with holograms and contents are instinctual.</span></span> <span data-ttu-id="4991a-199">Pour réaliser le mouvement instinctual consiste à guider les utilisateurs pour effectuer des mouvements de la conception d’intuitivité de l’interface utilisateur.</span><span class="sxs-lookup"><span data-stu-id="4991a-199">The way to achieve instinctual gesture is to guide users to perform gestures through the design of UI affordances.</span></span> <span data-ttu-id="4991a-200">Par exemple, si nous encourageons les utilisateurs à la capture d’un objet ou un point de contrôle avec pincement de deux doigts, l’objet ou le point de contrôle doit être petit.</span><span class="sxs-lookup"><span data-stu-id="4991a-200">For example, if we encourage users to grab an object or a control point with two finger pinch, the object or the control point should be small.</span></span> <span data-ttu-id="4991a-201">Si nous aimerions aux utilisateurs d’effectuer cinq manipulation doigt, l’objet ou le point de contrôle doit être relativement important.</span><span class="sxs-lookup"><span data-stu-id="4991a-201">If we would like users to perform five finger grab, the object or the control point should be relatively big.</span></span> <span data-ttu-id="4991a-202">Comme pour les boutons, un petit bouton limiterait les utilisateurs pour appuyer sur avec un seul doigt, tandis qu’un bouton énorme encourage les utilisateurs à appuyer sur avec leurs paumes.</span><span class="sxs-lookup"><span data-stu-id="4991a-202">Similar to buttons, a tiny button would limit users to press it with a single finger, while a huge button would encourage users to press it with their palms.</span></span>
![](images/Instinctual-Gestures-720px.jpg)<br>

## <a name="symmetric-design-between-hands-and-6-dof-controllers"></a><span data-ttu-id="4991a-203">Conception symétrique entre les mains et 6 contrôleurs DDL</span><span class="sxs-lookup"><span data-stu-id="4991a-203">Symmetric design between hands and 6 DoF controllers</span></span>
<span data-ttu-id="4991a-204">Vous avez peut-être remarqué qu’il existe désormais parallels interaction que nous pouvons dessiner entre les mains dans les contrôleurs AR et mouvement dans VR.</span><span class="sxs-lookup"><span data-stu-id="4991a-204">You may have noticed that there are now interaction parallels we can draw between hands in AR and motion controllers in VR.</span></span> <span data-ttu-id="4991a-205">Les deux entrées peuvent servir à déclencher les manipulations directes dans leurs environnements respectifs.</span><span class="sxs-lookup"><span data-stu-id="4991a-205">Both inputs can be used to trigger direct manipulations in their respective environments.</span></span> <span data-ttu-id="4991a-206">Dans les 2 HoloLens, faisant des mains au fonctionnement de la distance fermer une grande partie de la même façon que le bouton de manipulation glisser effectuent sur les contrôleurs de mouvement dans WMR.</span><span class="sxs-lookup"><span data-stu-id="4991a-206">In HoloLens 2, grabbing and dragging with hands at a close distance works much in the same way as the grab button does on the motion controllers in WMR.</span></span> <span data-ttu-id="4991a-207">Cela fournit à vos utilisateurs avec une bonne connaissance de l’interaction entre les deux plateformes et peut s’avérer utile devez jamais vous décidez de migrer votre application à partir d’un à l’autre.</span><span class="sxs-lookup"><span data-stu-id="4991a-207">This provides your users with interaction familiarity between the two platforms and may prove useful should you ever decide to port your app from one to the other.</span></span>

## <a name="optimizing-with-eye-tracking"></a><span data-ttu-id="4991a-208">Optimisation avec suivi de le œil</span><span class="sxs-lookup"><span data-stu-id="4991a-208">Optimizing with eye tracking</span></span>
<span data-ttu-id="4991a-209">Manipulation directe peut se sentent magique si elle fonctionne comme prévu, mais peut également rapidement devenir frustrante si vous ne pouvez pas déplacer n’importe où votre main plus sans par inadvertance déclencher un hologramme.</span><span class="sxs-lookup"><span data-stu-id="4991a-209">Direct manipulation can feel magical if it works as intended, but can also quickly become frustrating if you can’t move your hand anywhere anymore without unintentionally triggering a hologram.</span></span>
<span data-ttu-id="4991a-210">Suivi de le œil peut potentiellement utile dans une meilleure identification des éléments intention de l’utilisateur.</span><span class="sxs-lookup"><span data-stu-id="4991a-210">Eye tracking can potentially help in better identifying what the user’s intent is.</span></span> 

* <span data-ttu-id="4991a-211">**Lorsque**: Réduire faussement déclencher une réponse de manipulation.</span><span class="sxs-lookup"><span data-stu-id="4991a-211">**When**: Reduce falsely triggering a manipulation response.</span></span> <span data-ttu-id="4991a-212">Suivi de le œil permet de mieux comprendre qu’un utilisateur actuellement engagé avec.</span><span class="sxs-lookup"><span data-stu-id="4991a-212">Eye tracking allows for better understanding what a user is currently engaged with.</span></span> <span data-ttu-id="4991a-213">Par exemple, imaginez la que lecture dans un texte (démonstration) HOLOGRAPHIQUE lorsque vous atteignez plus vous attraper l’outil de travail réel.</span><span class="sxs-lookup"><span data-stu-id="4991a-213">For example, imagine you are reading through a holographic (instructional) text when reaching over to grab you real-world work tool.</span></span>
<span data-ttu-id="4991a-214">En procédant ainsi, vous accidentellement déplacer votre main sur certains boutons HOLOGRAPHIQUE interactifs qui vous auriez pas remarqué même avant (par exemple, qu'il était même en dehors du champ de vision de l’utilisateur).</span><span class="sxs-lookup"><span data-stu-id="4991a-214">By doing so, you accidently move your hand across some interactive holographic buttons that you hadn't even noticed before (maybe it even was outside of the user's Field-of-View).</span></span>
<span data-ttu-id="4991a-215">Pour faire court : Si l’utilisateur n’a pas examiné un hologramme pendant un certain temps, mais un événement tactile ou comprendre a été détecté pour celle-ci, il est probable que l’utilisateur n’a pas été réellement qui a l’intention d’interagir avec ce hologramme.</span><span class="sxs-lookup"><span data-stu-id="4991a-215">Long story short: If the user hasn't looked at a hologram for a while, yet a touch or grasp event has been detected for it, it is likely that the user wasn't actually intending to interact with that hologram.</span></span> 

* <span data-ttu-id="4991a-216">**Celui**: À part l’adressage false activations positif, un autre exemple comprend mieux identifier les hologrammes pour saisir ou examiner le point d’intersection précise ne peut pas être clair à partir de votre point de vue en particulier si plusieurs hologrammes sont positionnés proximité les uns autres.</span><span class="sxs-lookup"><span data-stu-id="4991a-216">**Which one**: Aside from addressing false positive activations, another example includes better identifying which holograms to grab or poke as the precise intersection point may not be clear from your perspective especially if several holograms are positioned close to each other.</span></span> <span data-ttu-id="4991a-217">Alors que suivi d’yeux sur HoloLens 2 a une certaine limite sur la façon de précisément, elle peut déterminer vous regards des yeux, elle peut être très utile pour près interactions en raison d’une disparité de profondeur lors de l’interaction avec la main d’entrée.</span><span class="sxs-lookup"><span data-stu-id="4991a-217">While eye tracking on HoloLens 2 has a certain limitation on how accurately it can determine you eye gaze, this can still be very helpful for near interactions due to depth disparity when interacting with hand input.</span></span> <span data-ttu-id="4991a-218">Cela signifie qu’il est parfois difficile de déterminer si votre main est derrière ou devant un hologramme précisément Attrapez par exemple un widget de manipulation.</span><span class="sxs-lookup"><span data-stu-id="4991a-218">This means that it is sometimes difficult to determine whether your hand is behind or in front of a hologram to precisely grab a manipulation widget for example.</span></span>

 * <span data-ttu-id="4991a-219">**Où**: Utilisez les informations qu’un utilisateur recherche des gestes levant rapides.</span><span class="sxs-lookup"><span data-stu-id="4991a-219">**Where to**: Use information about what a user is looking at with quick throwing gestures.</span></span> <span data-ttu-id="4991a-220">Saisissez un hologramme et à peu près jeter celui-ci vers votre destination prévue.</span><span class="sxs-lookup"><span data-stu-id="4991a-220">Grab a hologram and roughly toss it toward your intended destination.</span></span> <span data-ttu-id="4991a-221">Parfois, cela peut fonctionner parfaitement, effectuer rapidement les mouvements de main peut entraîner des destinations hautement inexactes.</span><span class="sxs-lookup"><span data-stu-id="4991a-221">While this may sometimes work just fine, quickly performing hand gestures may result in highly inaccurate destinations.</span></span>
<span data-ttu-id="4991a-222">Il s’agit où yeux peut aider à appuyer la main levée vecteur précédent de votre position prévue.</span><span class="sxs-lookup"><span data-stu-id="4991a-222">This is where eye tracking could help out to lean the hand throwing vector back to your intended position.</span></span>

## <a name="see-also"></a><span data-ttu-id="4991a-223">Voir aussi</span><span class="sxs-lookup"><span data-stu-id="4991a-223">See also</span></span>
* [<span data-ttu-id="4991a-224">Regards et validation</span><span class="sxs-lookup"><span data-stu-id="4991a-224">Gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="4991a-225">Point et validation</span><span class="sxs-lookup"><span data-stu-id="4991a-225">Point and commit</span></span>](point-and-commit.md)
* [<span data-ttu-id="4991a-226">Fonctionnalités de base des interactions</span><span class="sxs-lookup"><span data-stu-id="4991a-226">Interaction fundamentals</span></span>](interaction-fundamentals.md)

