---
title: Atelier de Vision par ordinateur d’applications pour les casques de réalité mixte sur CVPR 2019
description: Vue d’ensemble et planification des applications Vision par ordinateur pour les casques d’écouteurs de réalité mixte, à livrer à la Conférence CVPR le 2019 juin.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: événement, mode de recherche, CVPR, vision par ordinateur, recherche, HoloLens
ms.openlocfilehash: 89d79bcef77043564e51faada940d2c71a6005e4
ms.sourcegitcommit: 2f600e5ad00cd447b180b0f89192b4b9d86bbc7e
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 06/15/2019
ms.locfileid: "67148711"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a><span data-ttu-id="e7124-104">Vision par ordinateur des applications pour les casques de réalité mixte</span><span class="sxs-lookup"><span data-stu-id="e7124-104">Computer Vision Applications for Mixed Reality Headsets</span></span>

<span data-ttu-id="e7124-105">Organisée conjointement avec [CVPR 2019](http://cvpr2019.thecvf.com/)</span><span class="sxs-lookup"><span data-stu-id="e7124-105">Organized in conjunction with [CVPR 2019](http://cvpr2019.thecvf.com/)</span></span>

<span data-ttu-id="e7124-106">Long Beach (CA)</span><span class="sxs-lookup"><span data-stu-id="e7124-106">Long Beach (CA)</span></span>

<span data-ttu-id="e7124-107">17 juin 2019 (après-midi)-Hyatt Regency F</span><span class="sxs-lookup"><span data-stu-id="e7124-107">June 17, 2019 (Afternoon) - Hyatt Regency F</span></span>


## <a name="organizers"></a><span data-ttu-id="e7124-108">Organisateur</span><span class="sxs-lookup"><span data-stu-id="e7124-108">Organizers</span></span>
* <span data-ttu-id="e7124-109">Marc Pollefeys</span><span class="sxs-lookup"><span data-stu-id="e7124-109">Marc Pollefeys</span></span>
* <span data-ttu-id="e7124-110">Federica Bogo</span><span class="sxs-lookup"><span data-stu-id="e7124-110">Federica Bogo</span></span>
* <span data-ttu-id="e7124-111">Johannes Schönberger</span><span class="sxs-lookup"><span data-stu-id="e7124-111">Johannes Schönberger</span></span>
* <span data-ttu-id="e7124-112">Osman Ulusoy</span><span class="sxs-lookup"><span data-stu-id="e7124-112">Osman Ulusoy</span></span>

## <a name="overview"></a><span data-ttu-id="e7124-113">Vue d'ensemble</span><span class="sxs-lookup"><span data-stu-id="e7124-113">Overview</span></span>

![Image de l’tease](images/cvpr2019_teaser2.jpg)

<span data-ttu-id="e7124-115">Les casques de réalité mixte, tels que Microsoft HoloLens, deviennent de puissantes plateformes pour développer des applications de vision informatique.</span><span class="sxs-lookup"><span data-stu-id="e7124-115">Mixed reality headsets such as the Microsoft HoloLens are becoming powerful platforms to develop computer vision applications.</span></span> <span data-ttu-id="e7124-116">Le mode de recherche HoloLens permet à Computer Vision Research sur l’appareil en fournissant un accès à tous les flux de capteur d’images brutes, y compris la profondeur et l’IR.</span><span class="sxs-lookup"><span data-stu-id="e7124-116">HoloLens Research Mode enables computer vision research on device by providing access to all raw image sensor streams -- including depth and IR.</span></span> <span data-ttu-id="e7124-117">Étant donné que le mode de recherche est désormais disponible depuis le 2018 mai, nous commençons à voir plusieurs démonstrations et applications intéressantes développées pour HoloLens.</span><span class="sxs-lookup"><span data-stu-id="e7124-117">As Research Mode is now available since May 2018, we are starting to see several interesting demos and applications being developed for HoloLens.</span></span> 

<span data-ttu-id="e7124-118">L’objectif de cet atelier est de réunir les étudiants et les chercheurs intéressés par la vision informatique pour les applications de réalité mixte.</span><span class="sxs-lookup"><span data-stu-id="e7124-118">The goal of this workshop is to bring together students and researchers interested in computer vision for mixed reality applications.</span></span> <span data-ttu-id="e7124-119">L’atelier vous permettra de partager des démonstrations et des applications, et d’apprendre les uns avec les autres pour créer ou porter des applications à la réalité mixte.</span><span class="sxs-lookup"><span data-stu-id="e7124-119">The workshop will provide a venue to share demos and applications, and learn from each other to build or port applications to mixed reality.</span></span> 

<span data-ttu-id="e7124-120">Nous encourageons les soumissions de la reconnaissance des objets (centrés sur ego), de la main et des utilisateurs, de la reconnaissance des activités, de la reconstruction 3D, de la présentation des scènes, de la localisation basée sur les capteurs, de la navigation et bien plus encore.</span><span class="sxs-lookup"><span data-stu-id="e7124-120">We encourage submissions on the topics of (ego-centric) object recognition, hand and user tracking, activity recognition, SLAM, 3D reconstruction, scene understanding, sensor-based localization, navigation and more.</span></span>

## <a name="paper-submission"></a><span data-ttu-id="e7124-121">Envoi de documents</span><span class="sxs-lookup"><span data-stu-id="e7124-121">Paper Submission</span></span>
* <span data-ttu-id="e7124-122">Délai d’envoi du document: 17 mai</span><span class="sxs-lookup"><span data-stu-id="e7124-122">Paper submission deadline: May 17</span></span>
* <span data-ttu-id="e7124-123">Notification aux auteurs: 24 mai</span><span class="sxs-lookup"><span data-stu-id="e7124-123">Notification to authors: May 24</span></span>

<span data-ttu-id="e7124-124">Les envois de documents doivent utiliser le modèle CVPR et sont limités à 4 pages plus références.</span><span class="sxs-lookup"><span data-stu-id="e7124-124">Paper submissions should use the CVPR template and are limited to 4 pages plus references.</span></span> <span data-ttu-id="e7124-125">En outre, nous encourageons les auteurs à envoyer une vidéo présentant leur application.</span><span class="sxs-lookup"><span data-stu-id="e7124-125">In addition, we encourage the authors to submit a video showcasing their application.</span></span>
<span data-ttu-id="e7124-126">Notez que les envois de travaux précédemment publiés sont autorisés (y compris le travail accepté pour la Conférence principale CVPR 2019).</span><span class="sxs-lookup"><span data-stu-id="e7124-126">Note that submissions of previously published work are allowed (including work accepted to the main CVPR 2019 conference).</span></span> 

<span data-ttu-id="e7124-127">Les envois peuvent être téléchargés vers le CMT: https://cmt3.research.microsoft.com/CVFORMR2019</span><span class="sxs-lookup"><span data-stu-id="e7124-127">Submissions can be uploaded to the CMT: https://cmt3.research.microsoft.com/CVFORMR2019</span></span>

<span data-ttu-id="e7124-128">Un sous-ensemble de documents sera sélectionné pour une présentation orale au cours de l’atelier.</span><span class="sxs-lookup"><span data-stu-id="e7124-128">A subset of papers will be selected for oral presentation at the workshop.</span></span> <span data-ttu-id="e7124-129">Toutefois, nous encourageons fortement tous les auteurs à présenter leur travail pendant la session de démonstration.</span><span class="sxs-lookup"><span data-stu-id="e7124-129">However, we strongly encourage all the authors to present their work during the demo session.</span></span>


## <a name="schedule"></a><span data-ttu-id="e7124-130">Planification</span><span class="sxs-lookup"><span data-stu-id="e7124-130">Schedule</span></span>
* <span data-ttu-id="e7124-131">13:30-13:45: Notes de bienvenue et d’ouverture.</span><span class="sxs-lookup"><span data-stu-id="e7124-131">13:30-13:45: Welcome and Opening Remarks.</span></span>
* <span data-ttu-id="e7124-132">13:45-14:15: **Discours parlé**: Prof. Marc Pollefeys, ETH Zurich/Microsoft.</span><span class="sxs-lookup"><span data-stu-id="e7124-132">13:45-14:15: **Keynote talk**: Prof. Marc Pollefeys, ETH Zurich/Microsoft.</span></span> <span data-ttu-id="e7124-133">Titre : Egocentric Vision par ordinateur sur HoloLens.</span><span class="sxs-lookup"><span data-stu-id="e7124-133">Title: Egocentric Computer Vision on HoloLens.</span></span>
* <span data-ttu-id="e7124-134">14:15-14:45: **Discours parlé**: Prof. Kris Kitani, Carnegie Mellon University.</span><span class="sxs-lookup"><span data-stu-id="e7124-134">14:15-14:45: **Keynote talk**: Prof. Kris Kitani, Carnegie Mellon University.</span></span> <span data-ttu-id="e7124-135">Titre : L’activité egocentric et les prévisions de pose.</span><span class="sxs-lookup"><span data-stu-id="e7124-135">Title: Egocentric Activity and Pose Forecasting.</span></span>
* <span data-ttu-id="e7124-136">14:45-15:15: **Discours parlé**: RD. Yang Liu, Californie Institute of Technology.</span><span class="sxs-lookup"><span data-stu-id="e7124-136">14:45-15:15: **Keynote talk**: Dr. Yang Liu, California Institute of Technology.</span></span> <span data-ttu-id="e7124-137">Titre : Alimenter un Assistant cognitive pour les aveugles avec une réalité augmentée.</span><span class="sxs-lookup"><span data-stu-id="e7124-137">Title: Powering a Cognitive Assistant for the Blind with Augmented Reality.</span></span>
* <span data-ttu-id="e7124-138">15:15-16:15: Pauses café et démonstrations.</span><span class="sxs-lookup"><span data-stu-id="e7124-138">15:15-16:15: Coffee break and demos.</span></span>
* <span data-ttu-id="e7124-139">16:15-16:45: **Discours parlé**: Prof. Kristen Grau, Université du Texas, Université de l’IA/Facebook.</span><span class="sxs-lookup"><span data-stu-id="e7124-139">16:15-16:45: **Keynote talk**: Prof. Kristen Grauman, University of Texas at Austin/Facebook AI Research.</span></span> <span data-ttu-id="e7124-140">Titre : Interaction avec l’objet humain dans la vidéo de première personne.</span><span class="sxs-lookup"><span data-stu-id="e7124-140">Title: Human-object interaction in first-person video.</span></span>
* <span data-ttu-id="e7124-141">16:45-17:15: Présentations orales:</span><span class="sxs-lookup"><span data-stu-id="e7124-141">16:45-17:15: Oral presentations:</span></span>
    * <span data-ttu-id="e7124-142">L’inscription a fait une navigation orthopédique simple et autonome avec HoloLens.</span><span class="sxs-lookup"><span data-stu-id="e7124-142">Registration made easy - standalone orthopedic navigation with HoloLens.</span></span> <span data-ttu-id="e7124-143">FA.</span><span class="sxs-lookup"><span data-stu-id="e7124-143">F.</span></span> <span data-ttu-id="e7124-144">Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.</span><span class="sxs-lookup"><span data-stu-id="e7124-144">Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.</span></span>
    * <span data-ttu-id="e7124-145">Apprentissage de la stéréo en parcourant un HoloLens.</span><span class="sxs-lookup"><span data-stu-id="e7124-145">Learning stereo by walking around with a HoloLens.</span></span> <span data-ttu-id="e7124-146">MANUTENTION.</span><span class="sxs-lookup"><span data-stu-id="e7124-146">H.</span></span> <span data-ttu-id="e7124-147">Zhan, Y. Pekelny, O. Ulusoy.</span><span class="sxs-lookup"><span data-stu-id="e7124-147">Zhan, Y. Pekelny, O. Ulusoy.</span></span>
* <span data-ttu-id="e7124-148">17:15-17:30: Remarques finales.</span><span class="sxs-lookup"><span data-stu-id="e7124-148">17:15-17:30: Final Remarks.</span></span>
