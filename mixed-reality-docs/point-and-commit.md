---
title: Pointer et valider avec les mains
description: Vue d’ensemble du modèle d’entrée Pointer et valider
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: Réalité mixte, interaction, conception, hololens, mains, éloigné, pointer et valider
ms.openlocfilehash: 30f85d2bb455abab3a533e0a829b4fba8cea0a7a
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: fr-FR
ms.lasthandoff: 06/05/2019
ms.locfileid: "66402382"
---
# <a name="point-and-commit-with-hands"></a><span data-ttu-id="5dd4d-104">Pointer et valider avec les mains</span><span class="sxs-lookup"><span data-stu-id="5dd4d-104">Point and commit with hands</span></span>
<span data-ttu-id="5dd4d-105">Grâce au modèle d’entrée Pointer et valider avec les mains, les utilisateurs peuvent cibler, sélectionner et manipuler du contenu 2D et des objets 3D à distance.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-105">Point and commit with hands is an input model that enables users to target, select and manipulate 2D content and 3D objects in the distance.</span></span> <span data-ttu-id="5dd4d-106">Cette technique d’interaction « éloignée » est propre à la réalité mixte et ne constitue pas pour les humains une méthode d’interaction naturelle avec le monde réel.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-106">This "far" interaction technique is unique to mixed reality and is not a way humans naturally intereact with the real world.</span></span> <span data-ttu-id="5dd4d-107">Par exemple, dans le film de super-héros *X-Men*, le personnage [Magnéto](https://en.wikipedia.org/wiki/Magneto_(comics)) peut atteindre et manipuler des objets éloignés avec les mains.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-107">For example, in the super hero movie *X-Men*, the character [Magneto](https://en.wikipedia.org/wiki/Magneto_(comics)) is capable of reaching out and manipulating a far object in the distance with his hands.</span></span> <span data-ttu-id="5dd4d-108">Ce n’est pas quelque chose que les humains peuvent faire dans la réalité.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-108">This is not something humans can do in reality.</span></span> <span data-ttu-id="5dd4d-109">Dans HoloLens (AR) et dans la réalité mixte (VR), nous dotons les utilisateurs de ce pouvoir magique, éliminant ainsi les contraintes physiques du monde réel pour proposer une expérience agréable avec le contenu holographique et rendre les interactions plus efficaces.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-109">In both HoloLens (AR) and Mixed Reality (VR), we equip users with this magical power, breaking the physical constraint of the real world not only to have a delightful experience with holographic contents but also to make the interaction more effective and efficient.</span></span>

## <a name="device-support"></a><span data-ttu-id="5dd4d-110">Prise en charge des appareils</span><span class="sxs-lookup"><span data-stu-id="5dd4d-110">Device support</span></span>

<span data-ttu-id="5dd4d-111">Modèle d’entrée</span><span class="sxs-lookup"><span data-stu-id="5dd4d-111">Input model</span></span> | [<span data-ttu-id="5dd4d-112">HoloLens (1re génération)</span><span class="sxs-lookup"><span data-stu-id="5dd4d-112">HoloLens (1st gen)</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/hololens-hardware-details) | <span data-ttu-id="5dd4d-113">HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="5dd4d-113">HoloLens 2</span></span> | [<span data-ttu-id="5dd4d-114">Casques immersifs</span><span class="sxs-lookup"><span data-stu-id="5dd4d-114">Immersive headsets</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/immersive-headset-hardware-details) |
| ---------| -----| ----- | ---------|
<span data-ttu-id="5dd4d-115">Pointer et valider avec les mains</span><span class="sxs-lookup"><span data-stu-id="5dd4d-115">Point and commit with hands</span></span> | <span data-ttu-id="5dd4d-116">❌ Non pris en charge</span><span class="sxs-lookup"><span data-stu-id="5dd4d-116">❌ Not supported</span></span> | <span data-ttu-id="5dd4d-117">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="5dd4d-117">✔️ Recommended</span></span> | <span data-ttu-id="5dd4d-118">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="5dd4d-118">✔️ Recommended</span></span>

<span data-ttu-id="5dd4d-119">Le modèle d’entrée Pointer et valider, aussi appelé « hands far » (mains éloignées), est l’une des fonctionnalités récemment introduites qui exploitent le nouveau système de suivi des mains articulées.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-119">Point and commit, also known as hands far, is one of the new features that utilizes the new articulated hand-tracking system.</span></span> <span data-ttu-id="5dd4d-120">Il constitue également le principal modèle d’entrée sur les casques immersifs équipés de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-120">This input model is also the primary input model on immersive headsets through the use of motion controllers.</span></span>

## <a name="hand-rays"></a><span data-ttu-id="5dd4d-121">Rayon émanant de la main</span><span class="sxs-lookup"><span data-stu-id="5dd4d-121">Hand rays</span></span>

<span data-ttu-id="5dd4d-122">Sur HoloLens 2, nous avons créé un rayon qui sort du centre de la paume de la main.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-122">On HoloLens 2, we created a hand ray that shoots out from the center of a palm.</span></span> <span data-ttu-id="5dd4d-123">Ce rayon est traité comme une extension de la main.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-123">This ray is treated as an extension of the hand.</span></span> <span data-ttu-id="5dd4d-124">Un curseur en forme d’anneau apparaît à l’extrémité du rayon pour indiquer à quel emplacement le rayon croise un objet cible.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-124">A donut-shaped cursor is attached to the end of the ray to indicate the location where the ray intersects with a target object.</span></span> <span data-ttu-id="5dd4d-125">L’objet sur lequel le curseur atterrit peut alors recevoir des commandes gestuelles de la main.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-125">The object that the cursor lands on can then receive gestural commands from the hand.</span></span>

<span data-ttu-id="5dd4d-126">Les commandes gestuelles de base sont déclenchées par un clic dans l’air, effectué avec le pouce et l’index.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-126">This basic gestural command is triggered by using the thumb and index finger to perform the air-tap action.</span></span> <span data-ttu-id="5dd4d-127">En utilisant le rayon émanant de la main pour pointer et un clic dans l’air pour valider, les utilisateurs peuvent activer un bouton ou un lien hypertexte dans du contenu web.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-127">By using the hand ray to point and air tap to commit, users can activate a button or a hyperlink on a web content.</span></span> <span data-ttu-id="5dd4d-128">Des mouvements plus composites permettent aux utilisateurs de naviguer dans le contenu web et de manipuler des objets 3D à distance.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-128">With more composite gestures, users are capable of navigating web content and manipulating 3D objects from a distance.</span></span> <span data-ttu-id="5dd4d-129">La conception visuelle du rayon doit également réagir aux états de pointage et de validation décrits et illustrés ci-dessous :</span><span class="sxs-lookup"><span data-stu-id="5dd4d-129">The visual design of the hand ray should also react to these point and commit states, as described and shown below:</span></span> 

* <span data-ttu-id="5dd4d-130">Dans l’état de *pointage*, le rayon est une ligne en pointillés et le curseur un anneau.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-130">In the *pointing* state, the ray is a dash line and the cursor is a donut shape.</span></span>
* <span data-ttu-id="5dd4d-131">Dans l’état de *validation*, le rayon se transforme en ligne continue et le curseur est réduit à un point.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-131">In the *commit* state, the ray turns into a solid line and the cursor shrinks to a dot.</span></span>

![](images/Hand-Rays-720px.jpg)

## <a name="transition-between-near-and-far"></a><span data-ttu-id="5dd4d-132">Transition entre le contenu proche et éloigné</span><span class="sxs-lookup"><span data-stu-id="5dd4d-132">Transition between near and far</span></span>

<span data-ttu-id="5dd4d-133">Au lieu d’utiliser des mouvements spécifiques comme « pointer avec l’index » pour diriger le rayon, nous avons conçu celui-ci pour sortir du centre de la paume de la main. Nous libérons ainsi les cinq doigts qui peuvent être utilisés pour effectuer d’autres mouvements, comme pincer et saisir.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-133">Instead of using specific gesture, such as "pointing with index finger" to direct the ray, we designed the ray coming out from the center of the palm, releasing and reserving the five fingers for more manipulative gestures, such as pinch and grab.</span></span> <span data-ttu-id="5dd4d-134">Cette conception nous permet de créer un seul modèle mental prenant en charge le même ensemble de mouvements de la main pour les interactions avec du contenu proche ou éloigné.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-134">With this design, we create only one mental model, supporting exactly the same set of hand gestures for both near and far interaction.</span></span> <span data-ttu-id="5dd4d-135">Vous pouvez utiliser le même mouvement de saisie pour manipuler des objets à différentes distances.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-135">You can use the same grab gesture to manipulate objects at different distances.</span></span> <span data-ttu-id="5dd4d-136">L’appel des rayons est automatique et basé sur la proximité :</span><span class="sxs-lookup"><span data-stu-id="5dd4d-136">The invocation of the rays is automatic and proximity based:</span></span>

*  <span data-ttu-id="5dd4d-137">Quand un objet est à portée de bras (environ 50 cm), les rayons sont automatiquement désactivés pour encourager l’interaction proche.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-137">When an object is within arm reached distance (roughly 50 cm), the rays are turned off automatically encouraging for near interaction.</span></span>
*  <span data-ttu-id="5dd4d-138">Si l’objet se trouve à plus de 50 cm, les rayons sont activés.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-138">When the object is farther than 50 cm, the rays are turned on.</span></span> <span data-ttu-id="5dd4d-139">La transition doit être fluide et transparente.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-139">The transition should be smooth and seamless.</span></span>

![](images/Transition-Between-Near-And-Far-720px.jpg)

## <a name="2d-slate-interaction"></a><span data-ttu-id="5dd4d-140">Interaction avec une ardoise 2D</span><span class="sxs-lookup"><span data-stu-id="5dd4d-140">2D slate interaction</span></span>

<span data-ttu-id="5dd4d-141">Une ardoise 2D est un conteneur holographique hébergeant le contenu d’applications 2D comme un navigateur web.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-141">A 2D Slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="5dd4d-142">Le concept de l’interaction éloignée avec une ardoise 2D est le suivant : les utilisateurs se servent du rayon émanant de la main pour cibler un objet et effectuent un clic dans l’air pour le sélectionner.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-142">The design concept for far interacting with a 2D slate is to use hand rays to target and air tap to select.</span></span> <span data-ttu-id="5dd4d-143">Après avoir ciblé un élément à l’aide du rayon, ils peuvent cliquer dans l’air pour déclencher un lien hypertexte ou un bouton.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-143">After targeting with a hand ray, users can air tap to trigger a hyperlink or a button.</span></span> <span data-ttu-id="5dd4d-144">D’une main, ils peuvent faire un « clic dans l’air suivi d’un glissement » pour faire défiler le contenu de l’ardoise vers le haut et vers le bas.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-144">They can use one hand to "air tap and drag" to scroll a slate content up and down.</span></span> <span data-ttu-id="5dd4d-145">Le mouvement relatif des deux mains pour cliquer dans l’air et faire glisser permet de faire un zoom avant ou arrière sur le contenu de l’ardoise.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-145">The relative motion of using two hands to air tap and drag can zoom in and out the slate content.</span></span>

<span data-ttu-id="5dd4d-146">Le fait de cibler le rayon émanant de la main au niveau des coins et des bords permet de révéler l’affordance de manipulation la plus proche.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-146">Targeting the hand ray at the corners and edges reveals the closest manipulation affordance.</span></span> <span data-ttu-id="5dd4d-147">En « saisissant et en faisant glisser » les affordances de manipulation, les utilisateurs peuvent effectuer une mise à l’échelle uniforme (affordances de coin) et ajuster dynamiquement l’ardoise (affordances de bord).</span><span class="sxs-lookup"><span data-stu-id="5dd4d-147">By "grab and drag" the manipulation affordances, users can perform uniform scaling through the corner affordances and can reflow the slate via the edge affordances.</span></span> <span data-ttu-id="5dd4d-148">Pour déplacer l’ardoise entière, il suffit aux utilisateurs de saisir la barre holographique située en haut de l’ardoise 2D et de la faire glisser.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-148">Grabbing and dragging the holobar at the top of the 2D slate can users move the whole slate.</span></span>

![](images/2D-Slate-Interaction-Far-720px.jpg)

<span data-ttu-id="5dd4d-149">Pour manipuler l’ardoise 2D :</span><span class="sxs-lookup"><span data-stu-id="5dd4d-149">For manipulating the 2D slate itself:</span></span><br>

* <span data-ttu-id="5dd4d-150">Les utilisateurs pointent le rayon émanant de la main au niveau des coins ou des bords pour révéler l’affordance de manipulation la plus proche.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-150">Users point the hand ray at the corners or edges to reveal the closest manipulation affordance.</span></span> 
* <span data-ttu-id="5dd4d-151">En appliquant un mouvement de manipulation sur l’affordance, les utilisateurs peuvent effectuer une mise à l’échelle uniforme (affordance de coin) et ajuster dynamiquement l’ardoise (affordance de bord).</span><span class="sxs-lookup"><span data-stu-id="5dd4d-151">By applying a manipulation gesture on the affordance, users can perform uniform scaling through the corner affordance and can reflow the slate via the edge affordance.</span></span> 
* <span data-ttu-id="5dd4d-152">En appliquant un mouvement de manipulation sur la barre holographique située en haut de l’ardoise 2D, les utilisateurs peuvent déplacer l’ardoise entière.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-152">By applying a manipulation gesture on the holobar at the top of the 2D slate, users can move the whole slate.</span></span><br>

<br>

## <a name="3d-object-manipulation"></a><span data-ttu-id="5dd4d-153">Manipulation d’objets 3D</span><span class="sxs-lookup"><span data-stu-id="5dd4d-153">3D object manipulation</span></span>

<span data-ttu-id="5dd4d-154">En manipulation directe, les utilisateurs ont le choix entre deux méthodes pour manipuler un objet 3D : la manipulation basée sur l’affordance et la manipulation non basée sur l’affordance.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-154">In direct manipulation, there are two ways for users to manipulate 3D object, affordance-based manipulation and non-affordance based manipulation.</span></span> <span data-ttu-id="5dd4d-155">Dans le modèle Pointer et valider, les utilisateurs peuvent accomplir exactement les mêmes tâches à l’aide du rayon émanant de la main.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-155">In the point and commit model, users are capable of achieving exactly the same tasks through the hand rays.</span></span> <span data-ttu-id="5dd4d-156">Aucun apprentissage supplémentaire n’est nécessaire.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-156">No additional learning is needed.</span></span><br>

### <a name="affordance-based-manipulation"></a><span data-ttu-id="5dd4d-157">Manipulation basée sur l’affordance</span><span class="sxs-lookup"><span data-stu-id="5dd4d-157">Affordance-based manipulation</span></span>
<span data-ttu-id="5dd4d-158">Les utilisateurs utilisent le rayon émanant de la main pour pointer sur un objet et faire apparaître le cadre englobant et les affordances de manipulation.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-158">Users use hand rays to point and reveal the bounding box and manipulation affordances.</span></span> <span data-ttu-id="5dd4d-159">Les utilisateurs peuvent appliquer le mouvement de manipulation sur le cadre englobant pour déplacer l’objet entier, sur les affordances de bord pour faire pivoter l’objet et sur les affordances de coin pour effectuer une mise à l’échelle uniforme.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-159">Users can apply the manipulation gesture on the bounding box to move the whole object, on the edge affordances to rotate and on the coner affordances to scale uniformly.</span></span> <br>

![](images/3D-Object-Manipulation-Far-720px.jpg) <br>


### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="5dd4d-160">Manipulation non basée sur l’affordance</span><span class="sxs-lookup"><span data-stu-id="5dd4d-160">Non-affordance based manipulation</span></span>
<span data-ttu-id="5dd4d-161">Les utilisateurs pointent sur un objet à l’aide du rayon émanant de la main pour faire apparaître le cadre englobant, puis appliquent directement des mouvements de manipulation à cet objet.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-161">Users point with hand rays to reveal the bounding box then directly apply manipulation gestures on it.</span></span> <span data-ttu-id="5dd4d-162">Avec une main, la translation et la rotation de l’objet sont associées au mouvement et à l’orientation de la main.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-162">With one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="5dd4d-163">Avec les deux mains, les utilisateurs peuvent translater, mettre à l’échelle et faire tourner l’objet selon les mouvements relatifs des deux mains.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-163">With two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span><br>

<br>

## <a name="instinctual-gesturers"></a><span data-ttu-id="5dd4d-164">Mouvements instinctifs</span><span class="sxs-lookup"><span data-stu-id="5dd4d-164">Instinctual gesturers</span></span>
<span data-ttu-id="5dd4d-165">Le concept de mouvements instinctifs pour pointer et valider est similaire à celui de la manipulation directe.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-165">The concept of instinctual gestures for point and commit is similar to that for direct manipulation.</span></span> <span data-ttu-id="5dd4d-166">Les mouvements que les utilisateurs sont censés effectuer sur un objet 3D sont guidés par la conception des affordances de l’interface utilisateur.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-166">The gestures users are suppose to perform on a 3D object are guided by the design of UI affordances.</span></span> <span data-ttu-id="5dd4d-167">Par exemple, un petit point de contrôle peut inciter les utilisateurs à le pincer avec le pouce et l’index, tandis qu’un utilisateur peut souhaiter saisir un objet plus volumineux à l’aide de ses 5 doigts.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-167">For example, a small control point might motivate users to pinch with their thumb and index finger, while a user might want to grab a larger object using all 5 fingers.</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)<br>

## <a name="symmetric-design-between-hands-and-6-dof-controller"></a><span data-ttu-id="5dd4d-168">Conception symétrique entre les mains et le contrôleur 6DoF</span><span class="sxs-lookup"><span data-stu-id="5dd4d-168">Symmetric design between hands and 6 DoF controller</span></span> 
<span data-ttu-id="5dd4d-169">Le concept Pointer et valider pour l’interaction éloignée a été initialement créé et défini pour le portail de réalité mixte (MRP), dans lequel un utilisateur porte un casque immersif et interagit avec des objets 3D à l’aide de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-169">The concept of point and commit for far interaction was initially created and defined for the Mixed Reality Portal (MRP), where a user wears an immersive headset and interacts with 3D objects via motion controllers.</span></span> <span data-ttu-id="5dd4d-170">Les contrôleurs de mouvement émettent des rayons pour pointer et manipuler des objets éloignés.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-170">The motion controllers shoot out rays for pointing and manipulating far objects.</span></span> <span data-ttu-id="5dd4d-171">Les contrôleurs sont équipés de boutons pour valider différentes actions.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-171">There are buttons on the controllers for further committing different actions.</span></span> <span data-ttu-id="5dd4d-172">Nous exploitons le modèle d’interaction des rayons et les fixons aux deux mains.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-172">We leverage the interaction model of rays and attached them to both hands.</span></span> <span data-ttu-id="5dd4d-173">Grâce à cette conception symétrique, les utilisateurs qui sont familiarisés avec MRP ne sont pas contraints d’apprendre un autre modèle d’interaction pour pointer sur un objet éloigné et le manipuler quand ils utilisent HoloLens 2, et inversement.</span><span class="sxs-lookup"><span data-stu-id="5dd4d-173">With this symmetric design, users who are familiar with MRP won't need to learn another interaction model for far pointing and manipulation when they use HoloLen 2, and vice versa.</span></span>    

![](images/Symmetric-Design-For-Rays-720px.jpg)<br>

## <a name="instinctual-gestures"></a><span data-ttu-id="5dd4d-174">Mouvements instinctifs</span><span class="sxs-lookup"><span data-stu-id="5dd4d-174">Instinctual gestures</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)

## <a name="see-also"></a><span data-ttu-id="5dd4d-175">Voir également</span><span class="sxs-lookup"><span data-stu-id="5dd4d-175">See also</span></span>
* [<span data-ttu-id="5dd4d-176">Suivre de la tête et valider</span><span class="sxs-lookup"><span data-stu-id="5dd4d-176">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="5dd4d-177">Manipulation directe avec les mains</span><span class="sxs-lookup"><span data-stu-id="5dd4d-177">Direct manipulation with hands</span></span>](direct-manipulation.md)
* [<span data-ttu-id="5dd4d-178">Interactions instinctuelles</span><span class="sxs-lookup"><span data-stu-id="5dd4d-178">Instinctual interactions</span></span>](interaction-fundamentals.md)

