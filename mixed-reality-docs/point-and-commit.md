---
title: Point et validation
description: Vue d’ensemble du modèle d’entrée point et de validation
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/05/2019
ms.topic: article
keywords: Mixte réalité, interaction, concevoir
ms.openlocfilehash: e0e9c97053734ac0125fce40be7ffe9afbd2dd68
ms.sourcegitcommit: f5c1dedb3b9e29f27f627025b9e7613931a7ce18
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 04/27/2019
ms.locfileid: "64581309"
---
# <a name="point-and-commit"></a><span data-ttu-id="286e4-104">Point et validation</span><span class="sxs-lookup"><span data-stu-id="286e4-104">Point and commit</span></span>
<span data-ttu-id="286e4-105">Point et validation est un modèle d’entrée permet aux utilisateurs à cibler, sélectionner et manipuler le contenu 2D et 3D objets dans une distance.</span><span class="sxs-lookup"><span data-stu-id="286e4-105">Point and commit is an input model enables users to target, select and manipulate 2D contents and 3D objects in a distance.</span></span> <span data-ttu-id="286e4-106">Cette technique d’interaction de « Loin » est une expérience interactive le nombril étant humaine n’avions pas vraiment pendant leurs interactions quotidiennes avec le monde réel.</span><span class="sxs-lookup"><span data-stu-id="286e4-106">This "Far" interaction technique is a navel interactive experience that human being didn't really have during their daily interaction with the real world.</span></span> <span data-ttu-id="286e4-107">Par exemple, dans un film super héros, magnéto est capable de contacter et de manipulation d’un objet lointain via les mains dans une distance, mais humaines ne pouvez pas le faire en réalité.</span><span class="sxs-lookup"><span data-stu-id="286e4-107">For example, in a super hero movie, Magneto is capable of reaching out and manipulating a far object via hands in a distance, but human can't do it in reality.</span></span> <span data-ttu-id="286e4-108">Dans Microsoft HoloLens (AR) et réalité mixte de Microsoft (VR), nous doter les utilisateurs cette puissance magique, avec rupture de la contrainte physique du monde réel non seulement pour avoir une expérience optimale à vos de contenu HOLOGRAPHIQUE mais pour renforcer l’efficacité de l’interaction et efficace.</span><span class="sxs-lookup"><span data-stu-id="286e4-108">In both Microsoft HoloLens (AR) and Microsoft Mixed Reality (VR), we equip users this magical power, breaking the physical constraint of real world not only to have delightful experience with holographic contents but to make the interaction more effective and efficient.</span></span>

## <a name="device-support"></a><span data-ttu-id="286e4-109">Prise en charge des appareils</span><span class="sxs-lookup"><span data-stu-id="286e4-109">Device support</span></span>
<table>
    <colgroup>
    <col width="40%" />
    <col width="20%" />
    <col width="20%" />
    <col width="20%" />
    </colgroup>
    <tr>
        <td><span data-ttu-id="286e4-110"><strong>Modèle d’entrée</strong></span><span class="sxs-lookup"><span data-stu-id="286e4-110"><strong>Input model</strong></span></span></td>
        <td><span data-ttu-id="286e4-111"><a href="hololens-hardware-details.md"><strong>HoloLens (1er gen)</strong></a></span><span class="sxs-lookup"><span data-stu-id="286e4-111"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
        <td><span data-ttu-id="286e4-112"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="286e4-112"><strong>HoloLens 2</strong></span></span></td>
        <td><span data-ttu-id="286e4-113"><a href="immersive-headset-hardware-details.md"><strong>Casques IMMERSIFS</strong></a></span><span class="sxs-lookup"><span data-stu-id="286e4-113"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
    </tr>
     <tr>
        <td><span data-ttu-id="286e4-114">Point et validation (interaction lointain main)</span><span class="sxs-lookup"><span data-stu-id="286e4-114">Point and commit (far hand interaction)</span></span></td>
        <td><span data-ttu-id="286e4-115">❌ Ne pas pris en charge</span><span class="sxs-lookup"><span data-stu-id="286e4-115">❌ Not supported</span></span></td>
        <td><span data-ttu-id="286e4-116">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="286e4-116">✔️ Recommended</span></span></td>
        <td><span data-ttu-id="286e4-117">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="286e4-117">✔️ Recommended</span></span></td>
    </tr>
</table>
<br>
<span data-ttu-id="286e4-118">Point de validation a été l’un des modèles d’entrée principales sur HoloLens 2, utilisant la main articulée nouveau système de suivi.</span><span class="sxs-lookup"><span data-stu-id="286e4-118">Point and commit has been one of the primary input models on HoloLens 2, utilizing the new articulated hand tracking system.</span></span> <span data-ttu-id="286e4-119">Ce modèle d’entrée est également le modèle d’entrée principal sur des casques IMMERSIFS grâce à l’utilisation de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="286e4-119">This input model is also the primary input model on immersive headsets through the use of motion controllers.</span></span> <span data-ttu-id="286e4-120">Point et validation est le modèle d’entrée que nous suggérons pour remplacer l’utilisation de tête et de validation sur HoloLens (1er gen).</span><span class="sxs-lookup"><span data-stu-id="286e4-120">Point and Commit is the input model that we suggest to replace the Head Gaze and Commit on HoloLens (1st gen).</span></span> 

## <a name="hand-rays"></a><span data-ttu-id="286e4-121">Rayons de main</span><span class="sxs-lookup"><span data-stu-id="286e4-121">Hand rays</span></span>
<span data-ttu-id="286e4-122">Sur HoloLens 2, nous créons un rayon de main prise de vue à partir du centre d’un palm.</span><span class="sxs-lookup"><span data-stu-id="286e4-122">On HoloLens 2, we create a hand ray shooting out from the center of a palm.</span></span> <span data-ttu-id="286e4-123">Le rayon est traité comme une extension de la main.</span><span class="sxs-lookup"><span data-stu-id="286e4-123">The ray is treated as an extension of the hand.</span></span> <span data-ttu-id="286e4-124">Un curseur de forme de graphique en anneau est attaché à la fin du rayon à impliquent l’emplacement où le rayon en intersection avec un objet hitted.</span><span class="sxs-lookup"><span data-stu-id="286e4-124">A donut shape cursor is attached at the end of the ray to imply the location where the ray intersects with a hitted object.</span></span> <span data-ttu-id="286e4-125">L’objet accède le curseur reçoit les commandes de geste à partir de la main.</span><span class="sxs-lookup"><span data-stu-id="286e4-125">The object that the cursor lands will receive gestural commands from the hand.</span></span> 

<span data-ttu-id="286e4-126">La commande de geste très basique est déclenchée à l’aide de pouce et votre index pour effectuer le geste d’appui air.</span><span class="sxs-lookup"><span data-stu-id="286e4-126">The very basic gestural command is triggered by using thumb and index finger to perform air tap gesture.</span></span> <span data-ttu-id="286e4-127">À l’aide de ray de main pour pointer et appuyez sur validation, les utilisateurs peuvent activer un bouton ou un lien hypertexte sur un contenu web.</span><span class="sxs-lookup"><span data-stu-id="286e4-127">By using hand ray to point and air tap to commit, users can activate a button or a hyperlink on a web content.</span></span> <span data-ttu-id="286e4-128">Les gestes composites plus, les utilisateurs sont capables de navigation du contenu web et la manipulation des objets 3D dans une distance.</span><span class="sxs-lookup"><span data-stu-id="286e4-128">With more composite gestures, users are capable of navigating the web content and manipulating 3D objects in a distance.</span></span> <span data-ttu-id="286e4-129">La conception visuelle du rayon main doit réagir également pour pointer et valider des États :</span><span class="sxs-lookup"><span data-stu-id="286e4-129">The visual design of the hand ray should also react to point and commit states:</span></span> <br>
* <span data-ttu-id="286e4-130">Dans l’état de pointage, le rayon est dash inline, et le curseur se trouve une forme de graphique en anneau.</span><span class="sxs-lookup"><span data-stu-id="286e4-130">In the pointing state, the ray is dash lined, and the cursor is a donut shape.</span></span>
* <span data-ttu-id="286e4-131">dans l’état de validation, le rayon se transforme en une ligne pleine et le curseur est réduit à un point.</span><span class="sxs-lookup"><span data-stu-id="286e4-131">in the committing state, the ray turns into a solid line, and the cursor shrinks to a dot.</span></span><br><br>
![](images/Hand-Rays-720px.jpg)<br>

## <a name="transition-between-near-and-far"></a><span data-ttu-id="286e4-132">La transition entre près et lointain</span><span class="sxs-lookup"><span data-stu-id="286e4-132">Transition between near and far</span></span>
<span data-ttu-id="286e4-133">Au lieu d’utiliser des gestes spécifiques, par ex., pointant avec votre index pour diriger le rayon, nous concevons le rayon sort à partir du centre de palm, libérer et de réserver les cinq doigts pour plusieurs manipulations de geste.</span><span class="sxs-lookup"><span data-stu-id="286e4-133">Instead of using specific gestures, such as pointing with index finger to direct the ray, we design the ray coming out from the center of the palm, releasing and reserving the five fingers for more gestural manipulations.</span></span> <span data-ttu-id="286e4-134">Par conséquent, HoloLens 2 prend en charge exactement le même ensemble de mouvements de main pour l’interaction proche et lointain.</span><span class="sxs-lookup"><span data-stu-id="286e4-134">Therefore, HoloLens 2 supports exactly the same set of hand gestures for both near and far interaction.</span></span> <span data-ttu-id="286e4-135">Aucun apprentissage supplémentaire n’est nécessaire lorsque les utilisateurs de transit de trouve à proximité d’interactions lointain et vice versa.</span><span class="sxs-lookup"><span data-stu-id="286e4-135">No additional learning is needed when users transit from near to far interactions, and vice versa.</span></span> <span data-ttu-id="286e4-136">Utilisateurs peuvent utiliser le même mouvement de manipulation pour manipuler des objets à des distances différentes.</span><span class="sxs-lookup"><span data-stu-id="286e4-136">Users can use the same grab gesture to manipulate objects at different distances.</span></span> <span data-ttu-id="286e4-137">L’appel des rayons est automatique et proximité en fonction :</span><span class="sxs-lookup"><span data-stu-id="286e4-137">The invocation of the rays is automatic and proximity based:</span></span> <br>
* <span data-ttu-id="286e4-138">Lorsqu’un objet est dans arm atteint distance (environ 50 cm), les rayons sont désactivées automatiquement encourageant pour l’interaction proche.</span><span class="sxs-lookup"><span data-stu-id="286e4-138">when an object is within arm reached distance (roughly 50 cm), the rays are turned off automatically encouraging for near interaction.</span></span> 
* <span data-ttu-id="286e4-139">Lorsque l’objet est plus loin que 50 cm, les rayons sont sous tension.</span><span class="sxs-lookup"><span data-stu-id="286e4-139">When the object is farther than 50 cm, the rays are turned on.</span></span>

<span data-ttu-id="286e4-140">Ce mécanisme effectue la transition fluide et homogène.</span><span class="sxs-lookup"><span data-stu-id="286e4-140">This mechanism makes the transition smooth and seamless.</span></span><br>
![](images/Transition-Between-Near-And-Far-720px.jpg)<br>

## <a name="2d-slate-interaction"></a><span data-ttu-id="286e4-141">Interaction ardoise 2D</span><span class="sxs-lookup"><span data-stu-id="286e4-141">2D slate interaction</span></span>
<span data-ttu-id="286e4-142">Une ardoise 2D est un conteneur HOLOGRAPHIQUE hébergement de contenu application 2D, comme navigateur web.</span><span class="sxs-lookup"><span data-stu-id="286e4-142">A 2D slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="286e4-143">Le concept de conception pour présent interagir avec une ardoise 2D consiste à utiliser des rayons de main pour pointer et appuyez sur Valider.</span><span class="sxs-lookup"><span data-stu-id="286e4-143">The design concept for far interacting with a 2D slate is to use hand rays to point and air tap to commit.</span></span><br>

<span data-ttu-id="286e4-144">Pour interagir avec l’obtention de surfaces ardoise :</span><span class="sxs-lookup"><span data-stu-id="286e4-144">For interacting with the slate contant:</span></span><br>

* <span data-ttu-id="286e4-145">Les utilisateurs peuvent pointer vers un lien hypertexte ou un bouton, puis appui en l’air pour l’activer.</span><span class="sxs-lookup"><span data-stu-id="286e4-145">Users can point at a hyperlink or a button, then air tap to activate it.</span></span> 
* <span data-ttu-id="286e4-146">Utilisateurs peuvent utiliser une seule main pour effectuer un mouvement de navigation pour défiler un contenu ardoise.</span><span class="sxs-lookup"><span data-stu-id="286e4-146">Users can use one hand to perform a navigation gesture to scroll a slate content up and down.</span></span> 
* <span data-ttu-id="286e4-147">Utilisateurs peuvent utiliser deux mains pour effectuer des mouvements de navigation pour effectuer un zoom et de sortie le contenu ardoise.</span><span class="sxs-lookup"><span data-stu-id="286e4-147">Users can use two hands to perform navigation gestures to zoom in and out the slate content.</span></span><br><br>

![](images/2D-Slate-Interaction-Far-720px.jpg)<br>

<span data-ttu-id="286e4-148">Pour la manipulation 2D d’ardoise lui-même :</span><span class="sxs-lookup"><span data-stu-id="286e4-148">For manipulating the 2D slate itself:</span></span><br>

* <span data-ttu-id="286e4-149">Les utilisateurs pointent le rayon de la main à angles ou sur les bords pour révéler le plus proche intuitif de manipulation.</span><span class="sxs-lookup"><span data-stu-id="286e4-149">Users point the hand ray at the corners or edges to reveal the closest manipulation affordance.</span></span> 
* <span data-ttu-id="286e4-150">En appliquant un mouvement de manipulation sur l’intuitif, les utilisateurs peuvent effectuer une mise à l’échelle uniforme via intuitif de l’angle et peuvent ajuster dynamiquement l’ardoise via l’intuitif edge.</span><span class="sxs-lookup"><span data-stu-id="286e4-150">By applying a manipulation gesture on the affordance, users can perform uniform scaling through the corner affordance and can reflow the slate via the edge affordance.</span></span> 
* <span data-ttu-id="286e4-151">En appliquant un mouvement de manipulation sur la holobar en haut de l’ardoise 2D, les utilisateurs peuvent déplacer l’ardoise entière.</span><span class="sxs-lookup"><span data-stu-id="286e4-151">By applying a manipulation gesture on the holobar at the top of the 2D slate, users can move the whole slate.</span></span><br>

<br>

## <a name="3d-object-manipulation"></a><span data-ttu-id="286e4-152">Manipulation des objets 3D</span><span class="sxs-lookup"><span data-stu-id="286e4-152">3D object manipulation</span></span>
<span data-ttu-id="286e4-153">Dans manipulation directe, il existe deux façons pour les utilisateurs de manipuler l’objet 3D, intuitif basé Manipulation et Manipulation en fonction de Non-affordnace.</span><span class="sxs-lookup"><span data-stu-id="286e4-153">In direct manipulation, there are two ways for users to manipulate 3D object, Affordance Based Manipulation and Non-affordnace Based Manipulation.</span></span> <span data-ttu-id="286e4-154">Dans le modèle de point et de validation, les utilisateurs sont capables d’atteindre exactement les mêmes tâches par le biais des rayons de main.</span><span class="sxs-lookup"><span data-stu-id="286e4-154">In point and commit model, users are capable of achieving exactly the same tasks through the hand rays.</span></span> <span data-ttu-id="286e4-155">Aucun apprentissage supplémentaire n’est nécessaire.</span><span class="sxs-lookup"><span data-stu-id="286e4-155">No additional learning is needed.</span></span><br>

### <a name="affordance-based-manipulation"></a><span data-ttu-id="286e4-156">Manipulation d’intuitif basé</span><span class="sxs-lookup"><span data-stu-id="286e4-156">Affordance based manipulation</span></span>
<span data-ttu-id="286e4-157">Les utilisateurs utiliser rayons de main pour pointer et faire apparaître la zone englobante et l’intuitivité de manipulation.</span><span class="sxs-lookup"><span data-stu-id="286e4-157">Users use hand rays to point and reveal the bounding box and manipulation affordances.</span></span> <span data-ttu-id="286e4-158">Les utilisateurs peuvent appliquer le mouvement de manipulation sur la zone englobante pour déplacer l’objet entier, l’intuitivité edge pour faire pivoter et sur le coner permettant à l’échelle de manière uniforme.</span><span class="sxs-lookup"><span data-stu-id="286e4-158">Users can apply the manipulation gesture on the bounding box to move the whole object, on the edge affordances to rotate and on the coner affordances to scale uniformly.</span></span> <br>

![](images/3D-Object-Manipulation-Far-720px.jpg) <br>


### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="286e4-159">Le caractère non-intuitif en fonction de manipulation</span><span class="sxs-lookup"><span data-stu-id="286e4-159">Non-affordance based manipulation</span></span>
<span data-ttu-id="286e4-160">Les utilisateurs point avec des rayons de main pour faire apparaître la zone englobante, puis appliquer directement des mouvements de manipulation sur celui-ci.</span><span class="sxs-lookup"><span data-stu-id="286e4-160">Users point with hand rays to reveal the bounding box then directly apply manipulation gestures on it.</span></span> <span data-ttu-id="286e4-161">Une part, la traduction et la rotation de l’objet sont associées au mouvement et l’orientation de la main.</span><span class="sxs-lookup"><span data-stu-id="286e4-161">With one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="286e4-162">Avec deux mains, les utilisateurs peuvent traduire, mettre à l’échelle et faire pivoter en fonction des mouvements relatifs de deux mains.</span><span class="sxs-lookup"><span data-stu-id="286e4-162">With two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span><br>

<br>

## <a name="instinctual-gesturers"></a><span data-ttu-id="286e4-163">Gesturers instinctual</span><span class="sxs-lookup"><span data-stu-id="286e4-163">Instinctual gesturers</span></span>
<span data-ttu-id="286e4-164">Le concept de mouvements instinctual de point et de validation est synchronisé avec qui pour la manipulation directe.</span><span class="sxs-lookup"><span data-stu-id="286e4-164">The concept of instinctual gestures for point and commit is in sync with that for direct manipulation.</span></span> <span data-ttu-id="286e4-165">Ce que les utilisateurs de mouvements censés pour effectuer sur un objet 3D sont guidés par la conception du intuitivité de l’interface utilisateur.</span><span class="sxs-lookup"><span data-stu-id="286e4-165">What gestures users suppose to perform on a 3D object are guided by the design of UI affordances.</span></span> <span data-ttu-id="286e4-166">Un point de contrôle de petites serait motiver aux utilisateurs de pincement avec thumb et votre index, alors qu’un objet volumineux apporte aux utilisateurs de récupérer avec 5 doigt.</span><span class="sxs-lookup"><span data-stu-id="286e4-166">A small control point would motivate users to pinch with thumb and index finger, while a large object makes users to grab with 5 finger.</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)<br>

## <a name="symmetric-design-between-hands-and-6-dof-controller"></a><span data-ttu-id="286e4-167">Conception symétrique entre les mains et 6 contrôleur DDL</span><span class="sxs-lookup"><span data-stu-id="286e4-167">Symmetric design between hands and 6 DoF controller</span></span> 
<span data-ttu-id="286e4-168">Le concept de modèle de point et de validation pour l’interaction lointain est tout d’abord créé et défini pour le mixte réalité Portal (MRP), où les utilisateurs porter un casque immersif et interagissent avec l’objet 3d par le biais de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="286e4-168">The concept of point and commit model for far interaction is firstly created and defined for the Mixed Reality Portal (MRP), where users wear an immersive headset and interact with the 3d object via motion controllers.</span></span> <span data-ttu-id="286e4-169">Les contrôleurs de mouvement dépanner les rayons pour pointant et la manipulation des objets lointain.</span><span class="sxs-lookup"><span data-stu-id="286e4-169">The motion controllers shoot out rays for pointing and manipulating far objects.</span></span> <span data-ttu-id="286e4-170">Il existe des boutons sur les contrôleurs de validation supplémentaire de différentes fonctionnalités.</span><span class="sxs-lookup"><span data-stu-id="286e4-170">There are buttons on the controllers for further committing different functionalities.</span></span> <span data-ttu-id="286e4-171">Nous exploiter le modèle d’interaction des rayons et définissez-les sur deux mains.</span><span class="sxs-lookup"><span data-stu-id="286e4-171">We leverage the interaction model of rays and attach them on both hands.</span></span> <span data-ttu-id="286e4-172">Grâce à cette conception symétrique, les utilisateurs familiarisés avec MRP ne devront pas apprendre un autre modèle d’interaction pour pointant présent et la manipulation lors de la première fois à l’aide de HoloLen 2 et vice versa.</span><span class="sxs-lookup"><span data-stu-id="286e4-172">With this symmetric design, users who are familiar with MRP won't need to learn another interaction model for far pointing and manipulation while first time using HoloLen 2, and vice versa.</span></span>    

![](images/Symmetric-Design-For-Rays-720px.jpg)<br>


## <a name="see-also"></a><span data-ttu-id="286e4-173">Voir aussi</span><span class="sxs-lookup"><span data-stu-id="286e4-173">See also</span></span>
* [<span data-ttu-id="286e4-174">Regards et validation</span><span class="sxs-lookup"><span data-stu-id="286e4-174">Gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="286e4-175">Manipulation directe</span><span class="sxs-lookup"><span data-stu-id="286e4-175">Direct manipulation</span></span>](direct-manipulation.md)
* [<span data-ttu-id="286e4-176">Fonctionnalités de base des interactions</span><span class="sxs-lookup"><span data-stu-id="286e4-176">Interaction fundamentals</span></span>](interaction-fundamentals.md)
