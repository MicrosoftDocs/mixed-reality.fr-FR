---
title: Point et validation des mains
description: Vue d’ensemble du modèle d’entrée point et de validation
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: Une réalité, interaction, conception, hololens, mains, mixte présent, pointez et valider
ms.openlocfilehash: e69c8ff2091beff7d8fbbde4e6f24d909302290a
ms.sourcegitcommit: 1c0fbee8fa887525af6ed92174edc42c05b25f90
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 05/16/2019
ms.locfileid: "65730803"
---
# <a name="point-and-commit-with-hands"></a><span data-ttu-id="d4628-104">Point et validation des mains</span><span class="sxs-lookup"><span data-stu-id="d4628-104">Point and commit with hands</span></span>
<span data-ttu-id="d4628-105">Point et validation des mains est un modèle d’entrée qui permet aux utilisateurs à cibler, sélectionner et manipuler des objets de contenu et 3D 2D dans la distance.</span><span class="sxs-lookup"><span data-stu-id="d4628-105">Point and commit with hands is an input model that enables users to target, select and manipulate 2D content and 3D objects in the distance.</span></span> <span data-ttu-id="d4628-106">Cette technique d’interaction « beaucoup » est unique à la réalité mixte et n’est pas un humains moyen naturellement intereact avec le monde réel.</span><span class="sxs-lookup"><span data-stu-id="d4628-106">This "far" interaction technique is unique to mixed reality and is not a way humans naturally intereact with the real world.</span></span> <span data-ttu-id="d4628-107">Par exemple, dans le film super héros *X-hommes*, le caractère [magnéto](https://en.wikipedia.org/wiki/Magneto_(comics)) est capable de contacter et de manipulation d’un objet lointain dans la distance à ses mains.</span><span class="sxs-lookup"><span data-stu-id="d4628-107">For example, in the super hero movie *X-Men*, the character [Magneto](https://en.wikipedia.org/wiki/Magneto_(comics)) is capable of reaching out and manipulating a far object in the distance with his hands.</span></span> <span data-ttu-id="d4628-108">Cela n’est pas quelque chose l’homme faire en réalité.</span><span class="sxs-lookup"><span data-stu-id="d4628-108">This is not something humans can do in reality.</span></span> <span data-ttu-id="d4628-109">Dans les HoloLens (AR) et la réalité mixte (VR), nous équiper les internautes grâce à cette puissance magique, la contrainte physique du monde réel non seulement à une expérience optimale à vos avec contenu HOLOGRAPHIQUE, mais également pour améliorer l’interaction efficace et plus efficace.</span><span class="sxs-lookup"><span data-stu-id="d4628-109">In both HoloLens (AR) and Mixed Reality (VR), we equip users with this magical power, breaking the physical constraint of the real world not only to have a delightful experience with holographic contents but also to make the interaction more effective and efficient.</span></span>

## <a name="device-support"></a><span data-ttu-id="d4628-110">Prise en charge des appareils</span><span class="sxs-lookup"><span data-stu-id="d4628-110">Device support</span></span>

<span data-ttu-id="d4628-111">Modèle d’entrée</span><span class="sxs-lookup"><span data-stu-id="d4628-111">Input model</span></span> | [<span data-ttu-id="d4628-112">HoloLens (1er gen)</span><span class="sxs-lookup"><span data-stu-id="d4628-112">HoloLens (1st gen)</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/hololens-hardware-details) | <span data-ttu-id="d4628-113">HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="d4628-113">HoloLens 2</span></span> | [<span data-ttu-id="d4628-114">Casques IMMERSIFS</span><span class="sxs-lookup"><span data-stu-id="d4628-114">Immersive headsets</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/immersive-headset-hardware-details) |
| ---------| -----| ----- | ---------|
<span data-ttu-id="d4628-115">Point et validation (interaction lointain main)</span><span class="sxs-lookup"><span data-stu-id="d4628-115">Point and commit (far hand interaction)</span></span> | <span data-ttu-id="d4628-116">❌ Ne pas pris en charge</span><span class="sxs-lookup"><span data-stu-id="d4628-116">❌ Not supported</span></span> | <span data-ttu-id="d4628-117">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="d4628-117">✔️ Recommended</span></span> | <span data-ttu-id="d4628-118">✔️ Recommandé</span><span class="sxs-lookup"><span data-stu-id="d4628-118">✔️ Recommended</span></span>

<span data-ttu-id="d4628-119">Point et validation, également appelé mains présent, est une des nouvelles fonctionnalités qui utilise le nouveau système de suivi de la main articulé.</span><span class="sxs-lookup"><span data-stu-id="d4628-119">Point and commit, also known as hands far, is one of the new features that utilizes the new articulated hand-tracking system.</span></span> <span data-ttu-id="d4628-120">Ce modèle d’entrée est également le modèle d’entrée principal sur des casques IMMERSIFS grâce à l’utilisation de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="d4628-120">This input model is also the primary input model on immersive headsets through the use of motion controllers.</span></span>

## <a name="hand-rays"></a><span data-ttu-id="d4628-121">Rayons de main</span><span class="sxs-lookup"><span data-stu-id="d4628-121">Hand rays</span></span>

<span data-ttu-id="d4628-122">Sur HoloLens 2, nous avons créé un rayon de main qui enverra à partir du centre d’un palm.</span><span class="sxs-lookup"><span data-stu-id="d4628-122">On HoloLens 2, we created a hand ray that shoots out from the center of a palm.</span></span> <span data-ttu-id="d4628-123">Cette ray est traité comme une extension de la main.</span><span class="sxs-lookup"><span data-stu-id="d4628-123">This ray is treated as an extension of the hand.</span></span> <span data-ttu-id="d4628-124">Un curseur en forme de graphique en anneau est attaché à la fin du rayon utilisé pour indiquer l’emplacement où le rayon croise avec un objet cible.</span><span class="sxs-lookup"><span data-stu-id="d4628-124">A donut-shaped cursor is attached to the end of the ray to indicate the location where the ray intersects with a target object.</span></span> <span data-ttu-id="d4628-125">L’objet qui le curseur arrive sur peut ensuite recevoir des commandes geste à partir de la main.</span><span class="sxs-lookup"><span data-stu-id="d4628-125">The object that the cursor lands on can then receive gestural commands from the hand.</span></span>

<span data-ttu-id="d4628-126">Cette commande geste de base est déclenchée en utilisant le curseur de défilement et votre index pour effectuer l’action d’appui en l’air.</span><span class="sxs-lookup"><span data-stu-id="d4628-126">This basic gestural command is triggered by using the thumb and index finger to perform the air-tap action.</span></span> <span data-ttu-id="d4628-127">En utilisant le rayon de main pour pointer et appuyez sur validation, les utilisateurs peuvent activer un bouton ou un lien hypertexte sur un contenu web.</span><span class="sxs-lookup"><span data-stu-id="d4628-127">By using the hand ray to point and air tap to commit, users can activate a button or a hyperlink on a web content.</span></span> <span data-ttu-id="d4628-128">Les gestes composites plus, les utilisateurs sont capables de navigation de contenu web et la manipulation des objets 3D à partir d’une distance.</span><span class="sxs-lookup"><span data-stu-id="d4628-128">With more composite gestures, users are capable of navigating web content and manipulating 3D objects from a distance.</span></span> <span data-ttu-id="d4628-129">La conception visuelle du rayon main doit également réagir à ces États point et de validation, comme décrit et indiqué ci-dessous :</span><span class="sxs-lookup"><span data-stu-id="d4628-129">The visual design of the hand ray should also react to these point and commit states, as described and shown below:</span></span> 

* <span data-ttu-id="d4628-130">Dans le *pointant* d’état, le rayon est une ligne en tirets, et le curseur se trouve une forme de graphique en anneau.</span><span class="sxs-lookup"><span data-stu-id="d4628-130">In the *pointing* state, the ray is a dash line and the cursor is a donut shape.</span></span>
* <span data-ttu-id="d4628-131">Dans le *validation* d’état, le rayon se transforme en une ligne pleine et le curseur est réduit à un point.</span><span class="sxs-lookup"><span data-stu-id="d4628-131">In the *commit* state, the ray turns into a solid line and the cursor shrinks to a dot.</span></span>

![](images/Hand-Rays-720px.jpg)

## <a name="transition-between-near-and-far"></a><span data-ttu-id="d4628-132">La transition entre près et lointain</span><span class="sxs-lookup"><span data-stu-id="d4628-132">Transition between near and far</span></span>

<span data-ttu-id="d4628-133">Au lieu d’utiliser le mouvement spécifique, par ex., « pointant avec votre index » pour indiquer le rayon, nous avons conçu le rayon bientôt out à partir du centre de palm, libérer et de réserver les cinq doigts pour les gestes de manipulations plus, tel que pincement et récupérer.</span><span class="sxs-lookup"><span data-stu-id="d4628-133">Instead of using specific gesture, such as "pointing with index finger" to direct the ray, we designed the ray coming out from the center of the palm, releasing and reserving the five fingers for more manipulative gestures, such as pinch and grab.</span></span> <span data-ttu-id="d4628-134">Grâce à cette conception, nous ne créer qu’un seul modèle mental, prenant en charge exactement le même ensemble de mouvements de main pour l’interaction proche et lointain.</span><span class="sxs-lookup"><span data-stu-id="d4628-134">With this design, we create only one mental model, supporting exactly the same set of hand gestures for both near and far interaction.</span></span> <span data-ttu-id="d4628-135">Vous pouvez utiliser le même mouvement de manipulation pour manipuler des objets à des distances différentes.</span><span class="sxs-lookup"><span data-stu-id="d4628-135">You can use the same grab gesture to manipulate objects at different distances.</span></span> <span data-ttu-id="d4628-136">L’appel des rayons est automatique et proximité en fonction :</span><span class="sxs-lookup"><span data-stu-id="d4628-136">The invocation of the rays is automatic and proximity based:</span></span>

*  <span data-ttu-id="d4628-137">Lorsqu’un objet est dans arm atteint distance (environ 50 cm), les rayons sont désactivées automatiquement encourageant pour l’interaction proche.</span><span class="sxs-lookup"><span data-stu-id="d4628-137">When an object is within arm reached distance (roughly 50 cm), the rays are turned off automatically encouraging for near interaction.</span></span>
*  <span data-ttu-id="d4628-138">Lorsque l’objet est plus loin que 50 cm, les rayons sont sous tension.</span><span class="sxs-lookup"><span data-stu-id="d4628-138">When the object is farther than 50 cm, the rays are turned on.</span></span> <span data-ttu-id="d4628-139">La transition doit être fluide et homogène.</span><span class="sxs-lookup"><span data-stu-id="d4628-139">The transition should be smooth and seamless.</span></span>

![](images/Transition-Between-Near-And-Far-720px.jpg)

## <a name="2d-slate-interaction"></a><span data-ttu-id="d4628-140">Interaction ardoise 2D</span><span class="sxs-lookup"><span data-stu-id="d4628-140">2D slate interaction</span></span>

<span data-ttu-id="d4628-141">Une ardoise 2D est un conteneur HOLOGRAPHIQUE hébergement de contenu application 2D, comme navigateur web.</span><span class="sxs-lookup"><span data-stu-id="d4628-141">A 2D Slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="d4628-142">Le concept de conception pour présent interagir avec une ardoise 2D consiste à utiliser des rayons de main vers tap cible et air à sélectionner.</span><span class="sxs-lookup"><span data-stu-id="d4628-142">The design concept for far interacting with a 2D slate is to use hand rays to target and air tap to select.</span></span> <span data-ttu-id="d4628-143">Après la cible avec un rayon de main, les utilisateurs peuvent air tap pour déclencher un lien hypertexte ou un bouton.</span><span class="sxs-lookup"><span data-stu-id="d4628-143">After targeting with a hand ray, users can air tap to trigger a hyperlink or a button.</span></span> <span data-ttu-id="d4628-144">Ils peuvent utiliser une seule main pour « tap et faites glisser l’air » pour faire défiler un contenu ardoise diminué.</span><span class="sxs-lookup"><span data-stu-id="d4628-144">They can use one hand to "air tap and drag" to scroll a slate content up and down.</span></span> <span data-ttu-id="d4628-145">Le mouvement relatif de l’utilisation de deux mains pour appuyer et faire glisser l’air peut effectuer un zoom avant et de sortie le contenu ardoise.</span><span class="sxs-lookup"><span data-stu-id="d4628-145">The relative motion of using two hands to air tap and drag can zoom in and out the slate content.</span></span>

<span data-ttu-id="d4628-146">Le rayon disponible dans les angles et les bords de ciblage révèle le plus proche intuitif de manipulation.</span><span class="sxs-lookup"><span data-stu-id="d4628-146">Targeting the hand ray at the corners and edges reveals the closest manipulation affordance.</span></span> <span data-ttu-id="d4628-147">Par « manipulation et faites glisser » l’intuitivité de manipulation, les utilisateurs permettre effectuer des glyphes de largeurs uniformes mise à l’échelle via l’intuitivité de coin et peuvent ajuster dynamiquement l’ardoise via l’intuitivité edge.</span><span class="sxs-lookup"><span data-stu-id="d4628-147">By "grab and drag" the manipulation affordances, users can perform uniform scaling through the corner affordances and can reflow the slate via the edge affordances.</span></span> <span data-ttu-id="d4628-148">Les utilisateurs peuvent déplacer l’ardoise entière en saisissant et en faisant glisser le holobar en haut de l’ardoise 2D.</span><span class="sxs-lookup"><span data-stu-id="d4628-148">Grabbing and dragging the holobar at the top of the 2D slate can users move the whole slate.</span></span>

![](images/2D-Slate-Interaction-Far-720px.jpg)

<span data-ttu-id="d4628-149">Pour la manipulation 2D d’ardoise lui-même :</span><span class="sxs-lookup"><span data-stu-id="d4628-149">For manipulating the 2D slate itself:</span></span><br>

* <span data-ttu-id="d4628-150">Les utilisateurs pointent le rayon de la main à angles ou sur les bords pour révéler le plus proche intuitif de manipulation.</span><span class="sxs-lookup"><span data-stu-id="d4628-150">Users point the hand ray at the corners or edges to reveal the closest manipulation affordance.</span></span> 
* <span data-ttu-id="d4628-151">En appliquant un mouvement de manipulation sur l’intuitif, les utilisateurs peuvent effectuer une mise à l’échelle uniforme via intuitif de l’angle et peuvent ajuster dynamiquement l’ardoise via l’intuitif edge.</span><span class="sxs-lookup"><span data-stu-id="d4628-151">By applying a manipulation gesture on the affordance, users can perform uniform scaling through the corner affordance and can reflow the slate via the edge affordance.</span></span> 
* <span data-ttu-id="d4628-152">En appliquant un mouvement de manipulation sur la holobar en haut de l’ardoise 2D, les utilisateurs peuvent déplacer l’ardoise entière.</span><span class="sxs-lookup"><span data-stu-id="d4628-152">By applying a manipulation gesture on the holobar at the top of the 2D slate, users can move the whole slate.</span></span><br>

<br>

## <a name="3d-object-manipulation"></a><span data-ttu-id="d4628-153">Manipulation des objets 3D</span><span class="sxs-lookup"><span data-stu-id="d4628-153">3D object manipulation</span></span>

<span data-ttu-id="d4628-154">Dans manipulation directe, il existe deux moyens permettant aux utilisateurs de manipuler l’objet 3D, en fonction du intuitif de manipulation et manipulation en non intuitif.</span><span class="sxs-lookup"><span data-stu-id="d4628-154">In direct manipulation, there are two ways for users to manipulate 3D object, affordance-based manipulation and non-affordance based manipulation.</span></span> <span data-ttu-id="d4628-155">Dans le modèle de point et de validation, les utilisateurs sont capables d’atteindre exactement les mêmes tâches par le biais des rayons de main.</span><span class="sxs-lookup"><span data-stu-id="d4628-155">In the point and commit model, users are capable of achieving exactly the same tasks through the hand rays.</span></span> <span data-ttu-id="d4628-156">Aucun apprentissage supplémentaire n’est nécessaire.</span><span class="sxs-lookup"><span data-stu-id="d4628-156">No additional learning is needed.</span></span><br>

### <a name="affordance-based-manipulation"></a><span data-ttu-id="d4628-157">En fonction du intuitif de manipulation</span><span class="sxs-lookup"><span data-stu-id="d4628-157">Affordance-based manipulation</span></span>
<span data-ttu-id="d4628-158">Les utilisateurs utiliser rayons de main pour pointer et faire apparaître la zone englobante et l’intuitivité de manipulation.</span><span class="sxs-lookup"><span data-stu-id="d4628-158">Users use hand rays to point and reveal the bounding box and manipulation affordances.</span></span> <span data-ttu-id="d4628-159">Les utilisateurs peuvent appliquer le mouvement de manipulation sur la zone englobante pour déplacer l’objet entier, l’intuitivité edge pour faire pivoter et sur le coner permettant à l’échelle de manière uniforme.</span><span class="sxs-lookup"><span data-stu-id="d4628-159">Users can apply the manipulation gesture on the bounding box to move the whole object, on the edge affordances to rotate and on the coner affordances to scale uniformly.</span></span> <br>

![](images/3D-Object-Manipulation-Far-720px.jpg) <br>


### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="d4628-160">Le caractère non-intuitif en fonction de manipulation</span><span class="sxs-lookup"><span data-stu-id="d4628-160">Non-affordance based manipulation</span></span>
<span data-ttu-id="d4628-161">Les utilisateurs point avec des rayons de main pour faire apparaître la zone englobante, puis appliquer directement des mouvements de manipulation sur celui-ci.</span><span class="sxs-lookup"><span data-stu-id="d4628-161">Users point with hand rays to reveal the bounding box then directly apply manipulation gestures on it.</span></span> <span data-ttu-id="d4628-162">Une part, la traduction et la rotation de l’objet sont associées au mouvement et l’orientation de la main.</span><span class="sxs-lookup"><span data-stu-id="d4628-162">With one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="d4628-163">Avec deux mains, les utilisateurs peuvent traduire, mettre à l’échelle et faire pivoter en fonction des mouvements relatifs de deux mains.</span><span class="sxs-lookup"><span data-stu-id="d4628-163">With two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span><br>

<br>

## <a name="instinctual-gesturers"></a><span data-ttu-id="d4628-164">Gesturers instinctual</span><span class="sxs-lookup"><span data-stu-id="d4628-164">Instinctual gesturers</span></span>
<span data-ttu-id="d4628-165">Le concept de mouvements instinctual de point et de validation est semblable à la manipulation directe.</span><span class="sxs-lookup"><span data-stu-id="d4628-165">The concept of instinctual gestures for point and commit is similar to that for direct manipulation.</span></span> <span data-ttu-id="d4628-166">Les gestes que les utilisateurs sont censés pour effectuer sur un objet 3D sont guidés par la conception du intuitivité de l’interface utilisateur.</span><span class="sxs-lookup"><span data-stu-id="d4628-166">The gestures users are suppose to perform on a 3D object are guided by the design of UI affordances.</span></span> <span data-ttu-id="d4628-167">Par exemple, un point de contrôle petite peut motiver aux utilisateurs de pincement avec leurs thumb et le doigt de l’index, tandis qu’un utilisateur doit saisir un plus grand objet à l’aide de tous les doigts de 5.</span><span class="sxs-lookup"><span data-stu-id="d4628-167">For example, a small control point might motivate users to pinch with their thumb and index finger, while a user might want to grab a larger object using all 5 fingers.</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)<br>

## <a name="symmetric-design-between-hands-and-6-dof-controller"></a><span data-ttu-id="d4628-168">Conception symétrique entre les mains et 6 contrôleur DDL</span><span class="sxs-lookup"><span data-stu-id="d4628-168">Symmetric design between hands and 6 DoF controller</span></span> 
<span data-ttu-id="d4628-169">Le concept de point et de validation pour l’interaction lointain a été initialement créé et défini pour le mixte réalité Portal (MRP), où un utilisateur passionnés un casque immersif et interagit avec les objets 3D par le biais de contrôleurs de mouvement.</span><span class="sxs-lookup"><span data-stu-id="d4628-169">The concept of point and commit for far interaction was initially created and defined for the Mixed Reality Portal (MRP), where a user wears an immersive headset and interacts with 3D objects via motion controllers.</span></span> <span data-ttu-id="d4628-170">Les contrôleurs de mouvement dépanner les rayons pour pointant et la manipulation des objets lointain.</span><span class="sxs-lookup"><span data-stu-id="d4628-170">The motion controllers shoot out rays for pointing and manipulating far objects.</span></span> <span data-ttu-id="d4628-171">Il existe des boutons sur les contrôleurs de validation supplémentaire de différentes actions.</span><span class="sxs-lookup"><span data-stu-id="d4628-171">There are buttons on the controllers for further committing different actions.</span></span> <span data-ttu-id="d4628-172">Nous exploiter le modèle d’interaction des rayons et les joint à deux mains.</span><span class="sxs-lookup"><span data-stu-id="d4628-172">We leverage the interaction model of rays and attached them to both hands.</span></span> <span data-ttu-id="d4628-173">Grâce à cette conception symétrique, les utilisateurs familiarisés avec MRP ne devront pas apprendre un autre modèle d’interaction pour présent pointant et de manipulation lorsqu’ils utilisent HoloLen 2 et vice versa.</span><span class="sxs-lookup"><span data-stu-id="d4628-173">With this symmetric design, users who are familiar with MRP won't need to learn another interaction model for far pointing and manipulation when they use HoloLen 2, and vice versa.</span></span>    

![](images/Symmetric-Design-For-Rays-720px.jpg)<br>

## <a name="instinctual-gestures"></a><span data-ttu-id="d4628-174">Mouvements instinctual</span><span class="sxs-lookup"><span data-stu-id="d4628-174">Instinctual gestures</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)

## <a name="see-also"></a><span data-ttu-id="d4628-175">Voir aussi</span><span class="sxs-lookup"><span data-stu-id="d4628-175">See also</span></span>
* [<span data-ttu-id="d4628-176">Pointer du regard vers l’avant et valider</span><span class="sxs-lookup"><span data-stu-id="d4628-176">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="d4628-177">Manipulation directe avec les mains</span><span class="sxs-lookup"><span data-stu-id="d4628-177">Direct manipulation with hands</span></span>](direct-manipulation.md)
* [<span data-ttu-id="d4628-178">Interactions instinctuelles</span><span class="sxs-lookup"><span data-stu-id="d4628-178">Instinctual interactions</span></span>](interaction-fundamentals.md)

